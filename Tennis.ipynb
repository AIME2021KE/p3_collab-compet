{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: need to run it in Unity environment each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.29 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mjupyter-console 6.4.3 has requirement jupyter-client>=7.0.0, but you'll have jupyter-client 5.2.4 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.65278625 -1.5        -0.          0.\n",
      "  6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Description of provided files\n",
    "Despite having difficulties with the MADDPG running with the visual display and not a clear idea of what was \"good\" in terms of reward score, we've started with that set of code and files but putting the main.py 2 initial functions in the utilities.py file and taking the code of the main function out of the function and just provided inline below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic structure, operations\n",
    "We use a slightly modified MADDPG class that we had from the MADDPG mini-project, except now we change it to only have two interacting agents and to use separate Critic and Actor classes from the original Agent class now renamed DDPGAgent used in project 2 with some modifications. We also allow the state size and action sizes to be passed in to MADDPG along with some parameters we found useful in the past to modify during training tests. DDPGAgent in turn uses the Actor, Critic classes (along with ReplayBuffer and OUNoise helper classes) to define the details of the actor and critic local and target networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 First provide packages for this section for our solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KAE 4/10/2022:  this script modified from the main.py from the MADDPG mini-project\n",
    "# \n",
    "# here we have only 2 agents intead of 3 and both are cooperating\n",
    "#\n",
    "# we also don't anticipate needing images to save gifs, etc or log directories\n",
    "# we may need a keep_awake however.... since couldn't get it running locally\n",
    "#\n",
    "# main function that sets up environments\n",
    "# perform training loop\n",
    "\n",
    "from replaybuffer import ReplayBuffer\n",
    "from maddpg import MADDPG\n",
    "import torch\n",
    "import numpy as np\n",
    "from utilities import transpose_list, transpose_to_tensor, seeding, pre_process\n",
    "\n",
    "# keep training awake\n",
    "from workspace_utils import keep_awake\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Next initial various parameters for this run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor BatchNorm1d on, init....\n",
      "Actor BatchNorm1d on, init....\n",
      "Critic BatchNorm1d on, init....\n",
      "Critic BatchNorm1d on, init....\n",
      "DDPG Agent.init, BUFFER_SIZE: 100000\n",
      "DDPG Agent.init, BATCH_SIZE: 16\n",
      "DDPG Agent.init, WEIGHT_DECAY: 0\n",
      "DDPG Agent.init, UPDATE_EVERY: 5\n",
      "DDPG Agent.init, TIMES_UPDATE: 1\n",
      "OUNoise.reset, mu, sigma, theta, len: [ 0.  0.] 0.2 0.15 2\n",
      "Actor BatchNorm1d on, init....\n",
      "Actor BatchNorm1d on, init....\n",
      "Critic BatchNorm1d on, init....\n",
      "Critic BatchNorm1d on, init....\n",
      "DDPG Agent.init, BUFFER_SIZE: 100000\n",
      "DDPG Agent.init, BATCH_SIZE: 16\n",
      "DDPG Agent.init, WEIGHT_DECAY: 0\n",
      "DDPG Agent.init, UPDATE_EVERY: 5\n",
      "DDPG Agent.init, TIMES_UPDATE: 1\n",
      "OUNoise.reset, mu, sigma, theta, len: [ 0.  0.] 0.2 0.15 2\n"
     ]
    }
   ],
   "source": [
    "# amplitude of OU noise\n",
    "# this slowly decreases to 0\n",
    "#noise = 2\n",
    "#noise_reduction = 0.9999\n",
    "\n",
    "# initialize policy and critic\n",
    "#state_size\n",
    "#action_size\n",
    "random_seed = 123456765\n",
    "#lr_actor=1.0e-3\n",
    "#lr_critic=1.0e-2\n",
    "lr_actor=1.0e-4\n",
    "lr_critic=1.0e-3 # typically want critic learning higher than actor\n",
    "#lr_actor=1.0e-5\n",
    "#lr_critic=1.0e-4 # typically want critic learning higher than actor\n",
    "#lr_actor=3.0e-4\n",
    "#lr_critic=3.0e-3 # typically want critic learning higher than actor\n",
    "# KAE 4/13/2022: tried ranges in tau from 3e-2 to 1e-3 in 3x intervals for gamma 0.8, tau=1e-2 appeared best..\n",
    "#tau=1.0e-3\n",
    "tau=1.0e-2 # softmax mixing\n",
    "#tau=3.0e-2\n",
    "# KAE 4/13/2022: tried ranges in gamma from 0.5 to 0.99; 0.8 gamma with tau=1e-2 appeared best..\n",
    "gamma=0.8 # learning combination of Q's\n",
    "discount_factor=1.0 # reward discounting\n",
    "maddpg = MADDPG(state_size, action_size, random_seed, \\\n",
    "                lr_actor=lr_actor, lr_critic=lr_critic, tau=tau, \\\n",
    "                gamma=gamma, discount_factor=discount_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maddpg.set_noise(0.1, 0.9999, True)\n",
    "#maddpg.set_snoise(0.1, 0.9999, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize random number generator, now in utils, provides randome numbers \n",
    "#  seeds to numpy and torch\n",
    "seeding()\n",
    "\n",
    "# number of training episodes.\n",
    "# change this to higher number to experiment. say 30000.\n",
    "#number_of_episodes = 30000\n",
    "# initially start with a low value for checking things out, especially since \n",
    "#  can't see anything \n",
    "number_of_episodes = 150\n",
    "#number_of_episodes = 600\n",
    "#number_of_episodes = 1000\n",
    "#episode_length = 80\n",
    "max_episode_length = 1000\n",
    "\n",
    "# how many episodes to save policy and output\n",
    "#save_interval = 1000\n",
    "save_interval = 50\n",
    "\n",
    "save_interval = np.min([number_of_episodes, save_interval])\n",
    "\n",
    "avg_interval = np.min([number_of_episodes, save_interval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from collections import deque\n",
    "\n",
    "print_every=10\n",
    "moving_avg=100\n",
    "# KAE this is useful previously to provide a 'last moving_avg' sample for the moving average\n",
    "nscores_deque = deque(maxlen=moving_avg)\n",
    "nbest_mean = -100\n",
    "# KAE 3/19/2022: 1st time through we got learning score >36 but \n",
    "#  reloading gave us a mean of 34, so allow for 10% over\n",
    "MAX_SCORE = 0.55\n",
    "# KAE 3/19/2022: print the requirement score as well\n",
    "REQ_SCORE = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 Finally set the actual episode loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 with 15 iterations(emx:14, totmx:14)\tAverage Score: 0.0000, minact: -0.0365, maxact: 0.0499\n",
      "Episode 0, Average Score: 0.0000\n",
      "\n",
      "Mean score: 0.0000\t, old best: -100.0000, max_episode: 14\n",
      "Episode 10 with 167 iterations(emx:13, totmx:19)\tAverage Score: 0.0000, minact: -0.0343, maxact: 0.0297\n",
      "Episode 10, Average Score: 0.0000\n",
      "Episode 20 with 339 iterations(emx:20, totmx:20)\tAverage Score: 0.0000, minact: -0.0222, maxact: 0.0109\n",
      "Episode 20, Average Score: 0.0000\n",
      "Episode 30 with 571 iterations(emx:19, totmx:37)\tAverage Score: 0.0000, minact: -0.1043, maxact: 0.0904\n",
      "Episode 30, Average Score: 0.0000\n",
      "Episode 31 with 648 iterations(emx:76, totmx:76)\tAverage Score: 0.0063, minact: -0.1873, maxact: 0.1811\n",
      "Mean score: 0.0063\t, old best: 0.0000, max_episode: 76\n",
      "Episode 40 with 789 iterations(emx:14, totmx:76)\tAverage Score: 0.0049, minact: -0.3076, maxact: 0.3136\n",
      "Episode 40, Average Score: 0.0049\n",
      "Episode 41 with 808 iterations(emx:18, totmx:76)\tAverage Score: 0.0069, minact: -0.3787, maxact: 0.3839\n",
      "Mean score: 0.0069\t, old best: 0.0063, max_episode: 76\n",
      "Episode 44 with 856 iterations(emx:18, totmx:76)\tAverage Score: 0.0087, minact: -0.5940, maxact: 0.6010\n",
      "Mean score: 0.0087\t, old best: 0.0069, max_episode: 76\n",
      "Episode 50 with 941 iterations(emx:13, totmx:76)\tAverage Score: 0.0076, minact: -0.6334, maxact: 0.6466\n",
      "Episode 50, Average Score: 0.0076\n",
      "Episode 60 with 1083 iterations(emx:13, totmx:76)\tAverage Score: 0.0064, minact: -0.8153, maxact: 0.8258\n",
      "Episode 60, Average Score: 0.0064\n",
      "Episode 70 with 1225 iterations(emx:13, totmx:76)\tAverage Score: 0.0055, minact: -0.8600, maxact: 0.8676\n",
      "Episode 70, Average Score: 0.0055\n",
      "Episode 80 with 1367 iterations(emx:13, totmx:76)\tAverage Score: 0.0048, minact: -0.9582, maxact: 0.9626\n",
      "Episode 80, Average Score: 0.0048\n",
      "Episode 90 with 1509 iterations(emx:13, totmx:76)\tAverage Score: 0.0043, minact: -0.9803, maxact: 0.9824\n",
      "Episode 90, Average Score: 0.0043\n",
      "Episode 100 with 1651 iterations(emx:13, totmx:76)\tAverage Score: 0.0039, minact: -0.9898, maxact: 0.9908\n",
      "Episode 100, Average Score: 0.0039\n",
      "Episode 110 with 1793 iterations(emx:13, totmx:76)\tAverage Score: 0.0035, minact: -0.9894, maxact: 0.9884\n",
      "Episode 110, Average Score: 0.0035\n",
      "Episode 120 with 1935 iterations(emx:13, totmx:76)\tAverage Score: 0.0032, minact: -0.9917, maxact: 0.9911\n",
      "Episode 120, Average Score: 0.0032\n",
      "Episode 130 with 2077 iterations(emx:13, totmx:76)\tAverage Score: 0.0030, minact: -0.9940, maxact: 0.9938\n",
      "Episode 130, Average Score: 0.0030\n",
      "Episode 140 with 2219 iterations(emx:13, totmx:76)\tAverage Score: 0.0028, minact: -0.9958, maxact: 0.9968\n",
      "Episode 140, Average Score: 0.0028\n",
      "Episode 149 with 2347 iterations(emx:13, totmx:76)\tAverage Score: 0.0026, minact: -0.9945, maxact: 0.9960"
     ]
    }
   ],
   "source": [
    "# use keep_awake to keep workspace from disconnecting\n",
    "tstart = datetime.datetime.now()\n",
    "## KAE 4/12/22: needed?\n",
    "t = 0\n",
    "max_episode_t = -1\n",
    "nscores = []\n",
    "nmean_scores = []\n",
    "# KAE 4/12/2022: don't think we need for our application either agent0 or 1\n",
    "scores =  []\n",
    "#for episode in keep_awake(range(0, number_of_episodes, parallel_envs)):\n",
    "for episode in keep_awake(range(0, number_of_episodes)):\n",
    "\n",
    "    # KAE 4/11/2022: actual envrionment from above random setting to initalize the env_info\n",
    "    #  The states from the env_info\n",
    "    #  and the initialized scores\n",
    "    env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "    # KAE 4/11/2022: get the initial state for each agent....\n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    score = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "\n",
    "\n",
    "#    maddpg.reset() #\n",
    "    \n",
    "    #for calculating rewards for this particular episode - addition of all time steps\n",
    "    # save info or not\n",
    "    save_info = ((episode) % save_interval < 1 or episode==number_of_episodes-1)\n",
    "    tmax = 0\n",
    "    \n",
    "    episode_t = 0\n",
    "# temp values for monitoring action values....\n",
    "    min_action = 1.0e38\n",
    "    max_action = -1.0e38\n",
    "#    for episode_t in range(episode_length):\n",
    "    while True:\n",
    "        t += 1\n",
    "        # explore = only explore for a certain number of episodes\n",
    "        # action input needs to be transposed\n",
    "# KAE 4/12/22: get all actions (list) for each agent(2) in maddpg\n",
    "        actions = maddpg.act(states, add_noise=False)\n",
    "#        print('action, t',actions, t, episode)\n",
    "        min_actionc = np.min(actions)\n",
    "        max_actionc = np.max(actions)\n",
    "        min_action = np.min([min_action, min_actionc])\n",
    "        max_action = np.max([max_action, max_actionc])\n",
    "\n",
    "#        noise *= noise_reduction\n",
    "# KAE 4/12/22: maddpg.act is already clipped, so not needed here....\n",
    "        \n",
    "        # step forward one frame\n",
    "# KAE 4/12/22: previous env was custom made, here we are using the unity env which has a standard output\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = np.array(env_info.vector_observations)\n",
    "        rewards = np.array(env_info.rewards)\n",
    "        dones = np.array(env_info.local_done)\n",
    "        maddpg.step(states, actions, rewards, next_states, dones)\n",
    "        \n",
    "        score += rewards\n",
    "        states = next_states\n",
    "        \n",
    "#        maddpg.update_noise()\n",
    "        \n",
    "        if np.any(dones) or  episode_t > max_episode_length:\n",
    "            break \n",
    "        episode_t += 1\n",
    "\n",
    "\n",
    "    max_episode_t = np.max([max_episode_t, episode_t])\n",
    "    # update once after every episode_per_update\n",
    "    # KAE 4/12/22: this section is, for the most part, superceeded by the agent.step\n",
    "    #   function -> agent.learn for each agent\n",
    "    scores.append(np.max(score))\n",
    "\n",
    "    if episode % avg_interval == 0 or episode == number_of_episodes-1:\n",
    "        avg_rewards = np.mean(scores)\n",
    "\n",
    "    nscores_deque.append(scores)\n",
    "    nscores.append(scores)\n",
    "    mean_score = np.mean(nscores_deque)\n",
    "    # this gives us the mean score 100 sample mean score for plotting\n",
    "    nmean_scores.append(mean_score)\n",
    "    # THis line remains in place, from the DDPG previous example....\n",
    "#    print('\\rEpisode {}\\tAverage Score: {:.4f}'.format(episode, mean_score), end=\"\")\n",
    "    print('\\rEpisode {} with {} iterations(emx:{}, totmx:{})\\tAverage Score: {:.4f}, minact: {:.4f}, maxact: {:.4f}'.format(\n",
    "        episode, t, episode_t, max_episode_t, mean_score, min_action, max_action), end=\"\")\n",
    "    # only save the current score if it is better than previous save....\n",
    "    # document the ongoing process....\n",
    "    if episode % print_every == 0:\n",
    "        print('\\nEpisode {}, Average Score: {:.4f}'.format(episode, mean_score))\n",
    "    if mean_score > REQ_SCORE:\n",
    "        print('\\nRequirement met on Episode {}, Requirement Average Score: {:.4f}'.format(episode, mean_score))\n",
    "    if mean_score > MAX_SCORE:\n",
    "        print('\\nFinal Episode {}, Final Average Score: {:.4f}'.format(episode, mean_score))\n",
    "        break\n",
    "        \n",
    "    #saving model\n",
    "    save_dict_list =[]\n",
    "    if save_info or mean_score > nbest_mean:\n",
    "#        if save_info:\n",
    "#            print('\\nNormal save conditions...')\n",
    "        if mean_score > nbest_mean:\n",
    "#        else:\n",
    "            print('\\nMean score: {:.4f}\\t, old best: {:.4f}, max_episode: {}'.format(mean_score, nbest_mean, max_episode_t))\n",
    "            nbest_mean = mean_score\n",
    "        for i in range(num_agents):\n",
    "            save_dict = {'actor_params' : maddpg.maddpg_agent[i].actor_local.state_dict(),\n",
    "                         'actor_optim_params': maddpg.maddpg_agent[i].actor_optimizer.state_dict(),\n",
    "                         'critic_params' : maddpg.maddpg_agent[i].critic_local.state_dict(),\n",
    "                         'critic_optim_params' : maddpg.maddpg_agent[i].critic_optimizer.state_dict()}\n",
    "            save_dict_list.append(save_dict)\n",
    "\n",
    "#            torch.save(save_dict_list, 'checkpoint_episode-{}.pt'.format(episode))\n",
    "            torch.save(save_dict_list, 'checkpoint_episode-interim.pt')\n",
    "            \n",
    "torch.save(save_dict_list, 'checkpoint_episode-final.pt')\n",
    "\n",
    "# moved this up here as for some reason our solution stopped so lost the times\n",
    "tend = datetime.datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDPG learning took  0:00:17.945492 or  0.11963661333333335  seconds per episode\n"
     ]
    }
   ],
   "source": [
    "dtime = tend - tstart\n",
    "dt_e = float(dtime.total_seconds()) / float(number_of_episodes)\n",
    "print('DDPG learning took ',dtime,'or ',dt_e,' seconds per episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f69b8043470>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores,label='scores')\n",
    "plt.plot(np.arange(1, len(nmean_scores)+1), nmean_scores,'r-',\n",
    "         label='mean scores')\n",
    "plt.grid()\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: next section needs to be updated for the saved items above instead of P2 saved items...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KAE 3/19/2022: here we reload the saved trained models (both actor and critic)\n",
    "##   for a \"replay\" to demonstrate that our trained solution gives us \n",
    "##   good solutions (still)\n",
    "#agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "#agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
    "#\n",
    "## do only one \"episode\"\n",
    "#score = np.zeros(num_agents)\n",
    "#env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "#states = env_info.vector_observations\n",
    "#agent.reset()\n",
    "## keep track of the number of turns so can get a running count\n",
    "#icnt=0\n",
    "#while True:\n",
    "#    actions = agent.act(states, add_noise=False)\n",
    "#    env_info = env.step(actions)[brain_name]           # send all actions to the environment\n",
    "#    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#    dones = env_info.local_done                        # see if episode finished\n",
    "#\n",
    "#    states = next_states\n",
    "#    score += rewards                                  # update the score (for each agent)\n",
    "#    icnt += 1\n",
    "#    print('\\rTurn {}\\tRunning Mean Score: {:.2f}'.format(icnt, np.mean(score)), end=\"\")\n",
    "#    if np.any(dones):\n",
    "#        break \n",
    "#print('\\ncurrent test scores:',score)\n",
    "#print('\\ncurrent mean score:',np.mean(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: we don't uncomment the line below until things are working...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
