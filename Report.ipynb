{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: need to run it in Unity environment each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "tstart_run = datetime.datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.29 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mjupyter-console 6.4.3 has requirement jupyter-client>=7.0.0, but you'll have jupyter-client 5.2.4 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.65278625 -1.5        -0.          0.\n",
      "  6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Description of provided files\n",
    "I originally tried to base my model after the Udacity MADDPG (Multi-Agent Deep Deterministic Policy Gradient) lessons in the multi-agent actor-critic section for agent collaboration-competition, specifically the maddpg miniproject; however, I had issues with getting this to run with more than 3 non-zero rallies, so after looking on the web a several approaches:\n",
    "\n",
    "https://github.com/Nathan1123/P3_Collaboration_and_Competition\n",
    "https://github.com/ainvyu/p3-collab-compet\n",
    "\n",
    "I decided to abandon the MADDPG framework (which was mainly for a custom environment setup anyways), and work with a list of 2 of the DDPGAgents from the P2 / bipedal examples (I found the P2 DDPG Agent to be most useful). \n",
    "\n",
    "I started with those original ddpg_model.py and ddpg_agent.py files I had previously for the second project (P2) and modified them slightly, adding a few additional hyperparameters suggested by solutions I examined on the web and a corporate mentor.\n",
    "\n",
    "I did however take from the MADDPG miniproject the main.py 2 initial functions (seeing and preprocess) and placed them in the utilities.py file and taking the episode code of the main function out of the function and just provided inline below. However, we only use seeding (initialize the random number generators for numpy and torch)\n",
    "\n",
    "The main execution, plotting etc is all done in the Tennis.ipynb file.\n",
    "\n",
    "The rest of the files are support files from both MADDPG (workspace_utils.py functionality from the MADDPG mini-project in the hopes of avoiding any timeout issues with long runs, and the utilities.py file, adding the two functions at the beginning of main.py of that project to the utilities as well and two helper classes, replaybuffer and OUNoise, pulled out of the original P2 ddpg_agent.py, in part because the implementation of the MADDPG class (which we started with) had the replay buffer usage outside as a separate function, \n",
    "\n",
    "The Tennis.ipynb contains for the most part the original provided file, but with the original random actions code was removed and my code added in. There are several setup sections of code ahead of the main episode loop, but we didn't put it into a function (yet) to allow free flow of information in and out of the episode loop. The end contains the plots etc. We had planned to also test our results by reading in the saved models (commented out code) but ran out of time. It is removed in this report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My model description, used final values\n",
    "ddpg_model.py has two classes, an actor and a critic class, which are separately definable in terms of both the number of layers, noise additions, batchnormalization and/or dropout layers, and their sizes. However, for the vast majority of our efforts these two classes were essentially the same (hidden layer sizes and functionality).\n",
    "\n",
    "The NN in both the Actor and Critic is composed of 3 fully connected layers with two NN hidden layer sizes (defaults: fc1_size=***40*** and fc2_size=***20***) using RELU activation functions along with an initial state_size and a final action_size to map into the Tennis input (state) and output (action) environment via a tanh function for the actor, and the output of the final layer with 1 element for the critic. \n",
    "\n",
    "The DDPG has an __init__ function to be invoked on class creation and the forward method using the NN's to convert the current state into an action.\n",
    "\n",
    "Here we used the same structure we had for the DDPG for the pendulum problem, but based on the corporate mentor comments who had done it previously, we reduced the size of the layers to 20 and 40, respectively. However we fully explored a wide variety of layer size values from 400,300 to 20,20.\n",
    "\n",
    "Based on suggestions from the corporate mentor as well, we tried adding noise to the weights in the network, and then for good measure also tried it in the states as well as to the action. We also tried batch normalization, dropouts, etc but none of these seemed effective in training at the time that I looked at it (small BUFFERSIZE=16).\n",
    "\n",
    "The hyperparmeters passed in during intialization for the Actor and Critic networks are\n",
    "state_size -- the size of the state outputs (24)\n",
    "action_size - the size of the actions (2)\n",
    "seed -- a specified random seed, required 123456765\n",
    "fc1_units -- the size of the first hidden layer, default was a specified value in the model potentially independently for the actor and critic (40) \n",
    "fc2_units-- the size of the first hidden layer, default was a specified value in the model potentially independently for the actor and critic (20)\n",
    "momentum -- the value of the momentum for the batchnorm layer (currently unused) (0.1)\n",
    "\n",
    "Separate methods are provided for the Actor and Critic and include the basics, reset_parameters (used in init) to randomize the fully connected NN weights, and the forward function.\n",
    "\n",
    "The ddpg_agent.py file contains the DDPG Agent functionality, with both local and target networks for both the Actor and the Critic networks. The local networks do the continuous learning and the target networks are updated with those results slowly via soft_update method based on the size of the parameter tau. It also contains the optimizers for both the actor and the critic networks as well as the noise class and replaybuffer class functionality for this capability in the DDPG funtionality. We borrowed from \n",
    "\n",
    "https://github.com/Nathan1123/P3_Collaboration_and_Competition\n",
    "\n",
    "setting the memory to None and thereby only creating a (single) buffer for all DDPGAgent instances, so that the experiences of all the agents would be shared with each other.\n",
    "\n",
    "The input hyperparameters for ddpg_agent are:\n",
    "state_size -- dimension of each state (see above) (24)\n",
    "action_size -- dimension of each action (see above) (2)\n",
    "num_instance -- number of instances of agents employed (=1 for this project)\n",
    "random_seed -- random seed\n",
    "lr_actor -- learning rate of the actor (2.0e-4 final value)\n",
    "lr_critic -- learning rate of the critic (2.0e-4, final value)\n",
    "tau -- soft_update size of updating the target network with the local network values (1.0e-2 final value)\n",
    "gamma -- discount factor for the learn method for the critic network (0.8 final value)\n",
    "The ddpg_agent also contains the following hardcoded parameters:\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "WEIGHT_DECAY = 0        # L2 weight decay used in critic optimizer\n",
    "LEARN_START = 0         # minimum memory in replay buffer before executing learn method\n",
    "UPDATE_EVERY=10         # used to modulo the increasing t_step internal counter to determine if learning from replaybuffer experiences (or not)\n",
    "UPDATES_PER_STEP=10     # define number of times the model learns from the randomly selected experiences in replaybuffer\n",
    "NOISE_MU = 0.0          # the mean of the action noise for OUNoise class\n",
    "NOISE_THETA = 0.15      # the memory term for the Ornstein-Uhlenbeck process\n",
    "NOISE_SIGMA = 0.1       # the standard deviation of action noise for OUNoise class, 0.1 was better than 0.2\n",
    "\n",
    "The DDPGAgent has the usual agent functions of \n",
    "step\n",
    "act and \n",
    "learn, which in turn uses a soft_update function for updating the target network with local parameters based on the weighting tau\n",
    "\n",
    "The Agent class itself is composed of an __init__ fuction for construction, which creates the two actor-critic networks, one that is local and one that is the target network, along with the optimizer and memory buffer classes (separate file (package)) from the ReplayBuffer class to store experiences and a noise source (Orstein-Uhlenbeck, from the original DDPG mini-project) class as well. \n",
    "\n",
    "The Agent step method adds the current experience into the memory buffer for each agent, and stores the experience into memory. Step also exectutes the learn function various tests are passed:\n",
    "\n",
    "1) if the number of experiences exceeds the hyperparameter LEARN_START (here always 0)  \n",
    "2) for each call to step if the step count modulo the hyperparameter UPDATE_EVERY then the learn function can be called IF the experiences in replaymemory is larger than the BATCH_SIZE (which is the amount of experiences pulled out at a time)  \n",
    "3) Once the above two criteria are met, then perform UPDATES_PER_STEP number of learn steps for each call to the function step  \n",
    "\n",
    "The Agent act method returns actions for a given state given current policy. When it is not learning (no_grad()) it retrieves the actions for each actor from the local (actor) network and the current states. It does this by evaluating (eval) the local actor network, get new actions from the local network, train the local network if applicable, and finally select actions with noise.\n",
    "\n",
    "The Agent learn method was the one for which we had to provide the appropriate solutions previously with the DDPG P2 project, but with two instances in a list and one memory location. Here we unpack the tuple experiences into states, actions, rewards, next_states, and dones. The next_states are used in the target (NOT local) qnetwork to get the next target actions. These are then detached from the resulting tensor to make a true copy, access Qtable for the next action, and hence the rewards of the target network. We then get the next action results from the local network and then determine the MSE loss between the target and local network fits. We then zero_grad the optimizer, propagate the loss backwards through the network, and perform a step in the optimizer. Finally a soft update is performed on the target network, using TAU times the local network parameters and (1-TAU) times the target network parameters to update the target network parameters.\n",
    "\n",
    "As indicated the original DDPG, the agent has a helper class ReplayBuffer, with methods add, to add experiences to the buffer, and sample, to sample experiences from the buffer, and is used extensively in the step method for the Agent class.\n",
    "\n",
    "From the P2 project we retained the functionality of having multiple instances of games running, but for this effort we basically set the number of instances to 1. We have not tested if or how well having more instances would work (or not). For another day.\n",
    "\n",
    "Because I was (potentially) adding noise to the actions, I also needed to clip them before passing them out of act when that was the case (add_noise=True).\n",
    "\n",
    "Although we allowed for a delay to start learning while the replay buffer was filled, nothing we saw suggested anything else but zero (0) for this option and that is what we used throughout.\n",
    "\n",
    "The rest of the files are basically support functions; for example from the MADDPG miniproject we borrowed some of the utilties as is (workspace_utils.py, utilities.py), to keep the workspace alive on Udacity and to have some utility functions MADDPG, adding the seeding and preprocess functions from MADDPG main.py to the utility.py file, only using seeding function to start the random seeding of pytorch and numpy, however.\n",
    "\n",
    "The ReplayBuffer and OUNoise classes as separate .py files to keep things as clean as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPROACH\n",
    "After many failed attempts with the MADDPG class, I reviewed the below solutions:\n",
    "\n",
    "https://github.com/Nathan1123/P3_Collaboration_and_Competition\n",
    "https://github.com/ainvyu/p3-collab-compet\n",
    "\n",
    "which were quite simple but with large networks. \n",
    "\n",
    "From those reviews, I decided to simplify things (but leave the hooks in to show what I tried) and drop the MADDPG class. ALthough my approach is slightly different than either of these approaches, they were very helpful in focusing my next steps, especially just working with the DDPGAgents as a list of separate agents.\n",
    "\n",
    "I started with the default (intial) values for the previous DDPG except for the hidden layer sizes (used 20,20 (corporate mentor) and 400,300 (example cases), and then made changes to hyperparameters as I tested for sensitivity to those hyperparameters. I also tried later various suggestions from the corporate mentor, Udacity mentors, etc.\n",
    "\n",
    "The the Tennis.ipynb you will see that I create two separate instances of the ddpg_agents and loop through them in the processing, looking for max of their scores\n",
    "\n",
    "The biggest breakthrough aspect of our training was increasing the BUFFERSIZE from 16 to 128 (we also tried 64 and 256 buffer sizes  but 128 seemed to work best with our previously best \"searched for\" values). Although early on (with the MADDPG framework) increasing the buffersize didn't have much of any effect, with our simplier network implementation, all the larger sizes seemed to keep the learning going much longer, albiet in the other two cases the 100 sample moving average was more about 0.1 in about 4000-8000 episodes instead of the 0.5 we required\n",
    "\n",
    "We originally started with a higher gamma 0.9 and the previous values of the learning rates for the actor (1e-4) and critic (1e-3) but settled on the provided values in our final solution after our extensive earlier searches (but with the smaller buffersize). \n",
    "\n",
    "We searched on pretty much everything we could think of, including a number of approaches suggested by both mentors (increasing the hidden layer sizes, which was contrary to the corporate mentor suggestion to keep them small (20 and 20 was big enough)), ranging from 400,300 to 20,20. \n",
    "\n",
    "Similarly we examined 6 separate values of lra from 1e-3 to 1e-5 and 7 values of lrc from 2e-3 to 1e-5, although the search wasn't exhaustive. \n",
    "\n",
    "Values of gamma were explored from 0.8 to 0.99 and 5 values of tau from 1e-1 to 1e-3\n",
    "\n",
    "We also examined a number of suggestions from my corporate mentor (use parameter noise not action noise -- didn't seem to work at the time), and I looked briefly at adding state noise and dropout layers as well but eventually removed them as again they didn't seem to work and the prior examples I looked at didn't do that to get solutions, so I abandoned the effort.\n",
    "\n",
    "Another suggestion from my corporate mentor was to use a batchnorm1d after the first NN. I tried this as well but in the end I didn't see any improvement so I removed them as well, although I left in the hooks and commented code in case I want to generalize this capability in the future.\n",
    "\n",
    "Once I settled on the larger batch size (but with 20,20 hidden layer sizes), the results were promising but not sufficient so I started looking into increasing the hidden layer size and the 20,40 looked promising enough that I first set the episodes to exit after hitting the 0.5 mark, which it did shortly after 4000 iterations (8000 max) and then added a 10% buffer (0.55 before exit but print when it met the mark) and this took 7910 iterations (2.5 hrs to run!).\n",
    "\n",
    "Although normally I would like to do a little more hyperparameter searches to try to optimize, I ran out of time and with a solution (>0.5) I submitted it as is.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 First provide packages for this section for our solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After much trying SO many things to get my  network working, I looked up prior soluions (3 years old) on the net and found two potential sources for trying to see what I was doing wrong:\n",
    "https://github.com/ainvyu/p3-collab-compet\n",
    "https://github.com/Nathan1123/P3_Collaboration_and_Competition\n",
    "\n",
    "Although their solutions are different (Nathan's closest to mine in my structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KAE 4/10/2022:  this script modified from the main.py from the MADDPG mini-project\n",
    "# \n",
    "# here we have only 2 agents intead of 3 and both are cooperating\n",
    "#\n",
    "# we also don't anticipate needing images to save gifs, etc or log directories\n",
    "# we may need a keep_awake however.... since couldn't get it running locally\n",
    "#\n",
    "# main function that sets up environments\n",
    "# perform training loop\n",
    "\n",
    "from replaybuffer import ReplayBuffer\n",
    "#from maddpg import MADDPG\n",
    "from ddpg_agent import DDPGAgent\n",
    "import torch\n",
    "import numpy as np\n",
    "from utilities import transpose_list, transpose_to_tensor, seeding, pre_process\n",
    "\n",
    "# keep training awake\n",
    "from workspace_utils import keep_awake\n",
    "\n",
    "from collections import namedtuple, deque\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Next initial various parameters for this run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDPG Agent.init, BUFFER_SIZE: 100000\n",
      "DDPG Agent.init, BATCH_SIZE: 128\n",
      "DDPG Agent.init, WEIGHT_DECAY: 0\n",
      "DDPG Agent.init, UPDATE_EVERY: 10\n",
      "DDPG Agent.init, UPDATES_PER_STEP: 10\n",
      "DDPG Agent.init, LEARN_START: 0\n",
      "DDPG Agent.init, NOISE_MU: 0.0\n",
      "DDPG Agent.init, NOISE_THETA: 0.15\n",
      "DDPG Agent.init, NOISE_SIGMA: 0.1\n",
      "DDPG Agent.init, BUFFER_SIZE: 100000\n",
      "DDPG Agent.init, BATCH_SIZE: 128\n",
      "DDPG Agent.init, WEIGHT_DECAY: 0\n",
      "DDPG Agent.init, UPDATE_EVERY: 10\n",
      "DDPG Agent.init, UPDATES_PER_STEP: 10\n",
      "DDPG Agent.init, LEARN_START: 0\n",
      "DDPG Agent.init, NOISE_MU: 0.0\n",
      "DDPG Agent.init, NOISE_THETA: 0.15\n",
      "DDPG Agent.init, NOISE_SIGMA: 0.1\n"
     ]
    }
   ],
   "source": [
    "# amplitude of OU noise\n",
    "# this slowly decreases to 0\n",
    "#noise = 2\n",
    "#noise_reduction = 0.9999\n",
    "\n",
    "# initialize policy and critic\n",
    "#state_size\n",
    "#action_size\n",
    "random_seed = 123456765\n",
    "#lr_actor=1.0e-3\n",
    "#lr_critic=1.0e-2\n",
    "#lr_critic=1.0e-3 # typically want critic learning higher than actor, past experience\n",
    "#lr_actor=1.0e-4 # best so far, \n",
    "#lr_critic=1.0e-4 # with critic = actor, got lots of initial activity but petered out at end\n",
    "#lr_actor=1.0e-5 # pretty bad vs -4\n",
    "#lr_critic=1.0e-5 # pretty bad vs -4\n",
    "#lr_actor=1.0e-3 # best so far, better than -5 but worse than -4\n",
    "#lr_critic=1.0e-3 # with critic = actor, got lots of initial activity but petered out at end\n",
    "#lr_actor=1.0e-5\n",
    "#lr_actor=2.0e-4 # best so far, \n",
    "#lr_critic=2.0e-4 # with critic = actor, got lots of initial activity but petered out at end\n",
    "#lr_actor=0.5e-4 # worst, 1 peak\n",
    "#lr_critic=0.5e-4 # worst, 1 peak\n",
    "#lr_actor=4.0e-4 # poor\n",
    "#lr_critic=4.0e-4 # poor\n",
    "#lr_actor=2.0e-4 # bad\n",
    "#lr_critic=2.0e-3 # bad\n",
    "#lr_actor=2.0e-4 # bad\n",
    "#lr_critic=4.0e-4 # bad\n",
    "#lr_actor=0.5e-4 # worst yet\n",
    "#lr_critic=0.5e-4 # worst\n",
    "#lr_actor=0.5e-4 # late bloomer but poor\n",
    "#lr_critic=0.5e-3 # late bloomer but poor\n",
    "#lr_actor=1.0e-4 # late bloomer final 100 mean = 0.0461 (best) at 4000 its -- go further?\n",
    "#lr_critic=0.5e-3 # late bloomer final 100 mean = 0.0461 (best) at 4000 its\n",
    "#lr_actor=1.0e-4 # late bloomer final 100 mean = 0.0648 (best) at 4000 its -- go further?\n",
    "#lr_critic=1.0e-4 # late bloomer final 100 mean = 0.048 (best) at 4000 its\n",
    "lr_actor=2.0e-4 # late bloomer final 100 mean = 0.0648 (best) at 4000 its -- go further?\n",
    "lr_critic=2.0e-4 # late bloomer final 100 mean = 0.048 (best) at 4000 its\n",
    "#lr_actor=1.0e-4 # bad\n",
    "#lr_critic=0.2e-3 # bad\n",
    "#lr_critic=1.0e-4 # typically want critic learning higher than actor\n",
    "#lr_actor=3.0e-4\n",
    "#lr_critic=3.0e-3 # typically want critic learning higher than actor\n",
    "# KAE 4/13/2022: tried ranges in tau from 3e-2 to 1e-3 in 3x intervals for gamma 0.8, tau=1e-2 appeared best..\n",
    "#tau=1.0e-1\n",
    "#tau=3.0e-2\n",
    "#tau=3.0e-3\n",
    "#tau=3.0e-3\n",
    "tau=1.0e-2 # softmax mixing\n",
    "#tau=1.1e-2 # softmax mixing\n",
    "#tau=0.9e-2 # softmax mixing\n",
    "#tau=3.0e-2\n",
    "# KAE 4/13/2022: tried ranges in gamma from 0.5 to 0.99; 0.8 gamma with tau=1e-2 appeared best..\n",
    "gamma=0.8 # learning combination of Q's, best so far, 129-226 peaks\n",
    "#gamma=0.9 # learning combination of Q's, bad\n",
    "#gamma=0.99 # learning combination of Q's, really bad\n",
    "#gamma=0.7 # learning combination of Q's, worst\n",
    "#gamma=0.85 # learning combination of Q's, \n",
    "#gamma=0.82 # learning combination of Q's, \n",
    "#gamma=0.75 # learning combination of Q's, \n",
    "#gamma=0.81 # learning combination of Q's, 61 peaks max, bad\n",
    "gamma=0.79 # learning combination of Q's, 7 peaks max, really bad\n",
    "#discount_factor=1.0 # reward discounting\n",
    "num_instance = 1\n",
    "maddpg_agents = [ \n",
    "            DDPGAgent(state_size, action_size, num_instance, random_seed, \\\n",
    "                lr_actor=lr_actor, lr_critic=lr_critic, tau=tau, gamma=gamma), \\\n",
    "            DDPGAgent(state_size, action_size,num_instance, random_seed, \\\n",
    "                lr_actor=lr_actor, lr_critic=lr_critic, tau=tau, gamma=gamma) \n",
    "                ]\n",
    "\n",
    "\n",
    "#maddpg = MADDPG(state_size, action_size, random_seed, \\\n",
    "#                lr_actor=lr_actor, lr_critic=lr_critic, tau=tau, \\\n",
    "#                gamma=gamma, discount_factor=discount_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maddpg_agents[0].set_noise(0.1, 0.9999, True)\n",
    "#maddpg_agents[0].set_snoise(0.1, 0.9999, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize random number generator, now in utils, provides randome numbers \n",
    "#  seeds to numpy and torch\n",
    "seeding()\n",
    "\n",
    "# number of training episodes.\n",
    "# change this to higher number to experiment. say 30000.\n",
    "#number_of_episodes = 30000\n",
    "# initially start with a low value for checking things out, especially since \n",
    "#  can't see anything \n",
    "#number_of_episodes = 300\n",
    "#number_of_episodes = 600\n",
    "#number_of_episodes = 1000\n",
    "#number_of_episodes = 4000\n",
    "number_of_episodes = 8000 # THIS WAS THE ONE I TRIED LAST -- SUBMITTED\n",
    "#episode_length = 80\n",
    "max_episode_length = 2000\n",
    "\n",
    "# how many episodes to save policy and output\n",
    "#save_interval = 1000\n",
    "#save_interval = 50\n",
    "save_interval = 10\n",
    "\n",
    "save_interval = np.min([number_of_episodes, save_interval])\n",
    "\n",
    "avg_interval = np.min([number_of_episodes, save_interval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from collections import deque\n",
    "\n",
    "print_every=10\n",
    "moving_avg=100\n",
    "# KAE this is useful previously to provide a 'last moving_avg' sample for the moving average\n",
    "# KAE 3/19/2022: 1st time through we got learning score >36 but \n",
    "#  reloading gave us a mean of 34, so allow for 10% over\n",
    "MAX_SCORE = 0.55\n",
    "# KAE 3/19/2022: print the requirement score as well\n",
    "REQ_SCORE = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 Finally set the actual episode loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In DDPGAgent.reset noise\n",
      "In DDPGAgent.reset noise\n",
      "\n",
      "Episode 0\tAvg Score: 0.0000, dqmin: 0.0000, dqmax: 0.0000, DQ: 0.0000/-100.0000, pk: 0, mxpk: 0\n",
      "Mean score: 0.0000\t, old best: -100.0000, max_episode: 18\n",
      "\n",
      "Episode 10\tAvg Score: 0.0000, dqmin: 0.0000, dqmax: 0.0000, DQ: 0.0000/0.0000, pk: 0, mxpk: 0\n",
      "Episode 20\tAvg Score: 0.0000, dqmin: 0.0000, dqmax: 0.0000, DQ: 0.0000/0.0000, pk: 0, mxpk: 0\n",
      "Episode 30\tAvg Score: 0.0000, dqmin: 0.0000, dqmax: 0.0000, DQ: 0.0000/0.0000, pk: 0, mxpk: 0\n",
      "Episode 40\tAvg Score: 0.0000, dqmin: 0.0000, dqmax: 0.0000, DQ: 0.0000/0.0000, pk: 0, mxpk: 0\n",
      "Episode 50\tAvg Score: 0.0000, dqmin: 0.0000, dqmax: 0.0000, DQ: 0.0000/0.0000, pk: 0, mxpk: 0\n",
      "Episode 60\tAvg Score: 0.0000, dqmin: 0.0000, dqmax: 0.0000, DQ: 0.0000/0.0000, pk: 0, mxpk: 0\n",
      "Episode 70\tAvg Score: 0.0000, dqmin: 0.0000, dqmax: 0.0000, DQ: 0.0000/0.0000, pk: 0, mxpk: 0\n",
      "Episode 80\tAvg Score: 0.0000, dqmin: 0.0000, dqmax: 0.0000, DQ: 0.0000/0.0000, pk: 0, mxpk: 0\n",
      "Episode 90\tAvg Score: 0.0000, dqmin: 0.0000, dqmax: 0.0000, DQ: 0.0000/0.0000, pk: 0, mxpk: 0\n",
      "Episode 100\tAvg Score: 0.0000, dqmin: 0.0000, dqmax: 0.0000, DQ: 0.0000/0.0000, pk: 0, mxpk: 0\n",
      "Episode 110\tAvg Score: 0.0000, dqmin: 0.0000, dqmax: 0.0000, DQ: 0.0000/0.0000, pk: 0, mxpk: 0\n",
      "Episode 120\tAvg Score: 0.0000, dqmin: 0.0000, dqmax: 0.0000, DQ: 0.0000/0.0000, pk: 0, mxpk: 0\n",
      "Episode 130\tAvg Score: 0.0000, dqmin: 0.0000, dqmax: 0.0000, DQ: 0.0000/0.0000, pk: 0, mxpk: 0\n",
      "Episode 140\tAvg Score: 0.0000, dqmin: 0.0000, dqmax: 0.0000, DQ: 0.0000/0.0000, pk: 0, mxpk: 0\n",
      "Episode 150\tAvg Score: 0.0000, dqmin: 0.0000, dqmax: 0.0000, DQ: 0.0000/0.0000, pk: 0, mxpk: 0\n",
      "Episode 160\tAvg Score: 0.0000, dqmin: 0.0000, dqmax: 0.0000, DQ: 0.0000/0.0000, pk: 0, mxpk: 0\n",
      "Episode 170\tAvg Score: 0.0000, dqmin: 0.0000, dqmax: 0.0000, DQ: 0.0000/0.0000, pk: 0, mxpk: 0\n",
      "Episode 180\tAvg Score: 0.0000, dqmin: 0.0000, dqmax: 0.0000, DQ: 0.0000/0.0000, pk: 0, mxpk: 0\n",
      "Episode 190\tAvg Score: 0.0000, dqmin: 0.0000, dqmax: 0.0000, DQ: 0.0000/0.0000, pk: 0, mxpk: 0\n",
      "Episode 200\tAvg Score: 0.0000, dqmin: 0.0000, dqmax: 0.0000, DQ: 0.0000/0.0000, pk: 0, mxpk: 0\n",
      "Episode 210\tAvg Score: 0.0000, dqmin: 0.0000, dqmax: 0.0000, DQ: 0.0000/0.0000, pk: 0, mxpk: 0\n",
      "Episode 220\tAvg Score: 0.0000, dqmin: 0.0000, dqmax: 0.0000, DQ: 0.0000/0.0000, pk: 0, mxpk: 0\n",
      "Episode 230\tAvg Score: 0.0000, dqmin: 0.0000, dqmax: 0.0000, DQ: 0.0000/0.0000, pk: 0, mxpk: 0\n",
      "Mean score: 0.0010\t, old best: 0.0000, max_episode: 47\n",
      "\n",
      "Episode 240\tAvg Score: 0.0004, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0010/0.0010, pk: 1, mxpk: 1\n",
      "Episode 250\tAvg Score: 0.0008, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0020/0.0010, pk: 2, mxpk: 2\n",
      "Mean score: 0.0020\t, old best: 0.0010, max_episode: 47\n",
      "\n",
      "Episode 260\tAvg Score: 0.0019, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0050/0.0020, pk: 5, mxpk: 5\n",
      "Mean score: 0.0050\t, old best: 0.0020, max_episode: 47\n",
      "\n",
      "Episode 270\tAvg Score: 0.0022, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0060/0.0050, pk: 6, mxpk: 6\n",
      "Mean score: 0.0060\t, old best: 0.0050, max_episode: 47\n",
      "\n",
      "Episode 280\tAvg Score: 0.0025, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0070/0.0060, pk: 7, mxpk: 7\n",
      "Mean score: 0.0070\t, old best: 0.0060, max_episode: 47\n",
      "\n",
      "Episode 290\tAvg Score: 0.0024, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0070/0.0070, pk: 7, mxpk: 7\n",
      "Episode 300\tAvg Score: 0.0023, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0070/0.0070, pk: 7, mxpk: 7\n",
      "Episode 310\tAvg Score: 0.0026, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0080/0.0070, pk: 8, mxpk: 8\n",
      "Mean score: 0.0080\t, old best: 0.0070, max_episode: 47\n",
      "\n",
      "Episode 320\tAvg Score: 0.0031, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0100/0.0080, pk: 10, mxpk: 10\n",
      "Mean score: 0.0100\t, old best: 0.0080, max_episode: 48\n",
      "\n",
      "Episode 330\tAvg Score: 0.0048, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0160/0.0100, pk: 16, mxpk: 16\n",
      "Mean score: 0.0160\t, old best: 0.0100, max_episode: 48\n",
      "\n",
      "Episode 340\tAvg Score: 0.0067, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0220/0.0160, pk: 23, mxpk: 23\n",
      "Mean score: 0.0220\t, old best: 0.0160, max_episode: 48\n",
      "\n",
      "Episode 350\tAvg Score: 0.0074, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0240/0.0220, pk: 26, mxpk: 26\n",
      "Mean score: 0.0240\t, old best: 0.0220, max_episode: 48\n",
      "\n",
      "Episode 360\tAvg Score: 0.0089, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0270/0.0240, pk: 32, mxpk: 32\n",
      "Mean score: 0.0270\t, old best: 0.0240, max_episode: 48\n",
      "\n",
      "Episode 370\tAvg Score: 0.0086, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0260/0.0270, pk: 32, mxpk: 32\n",
      "Episode 380\tAvg Score: 0.0097, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0298/0.0270, pk: 37, mxpk: 35\n",
      "Mean score: 0.0298\t, old best: 0.0270, max_episode: 53\n",
      "\n",
      "Episode 390\tAvg Score: 0.0114, dqmin: 0.0000, dqmax: 0.1900, DQ: 0.0374/0.0298, pk: 44, mxpk: 38\n",
      "Mean score: 0.0374\t, old best: 0.0298, max_episode: 65\n",
      "\n",
      "Episode 400\tAvg Score: 0.0130, dqmin: 0.0000, dqmax: 0.1900, DQ: 0.0453/0.0374, pk: 52, mxpk: 38\n",
      "Mean score: 0.0453\t, old best: 0.0374, max_episode: 65\n",
      "\n",
      "Episode 410\tAvg Score: 0.0146, dqmin: 0.0000, dqmax: 0.1900, DQ: 0.0519/0.0453, pk: 60, mxpk: 38\n",
      "Mean score: 0.0519\t, old best: 0.0453, max_episode: 65\n",
      "\n",
      "Episode 420\tAvg Score: 0.0159, dqmin: 0.0000, dqmax: 0.1900, DQ: 0.0569/0.0519, pk: 67, mxpk: 38\n",
      "Mean score: 0.0569\t, old best: 0.0519, max_episode: 65\n",
      "\n",
      "Episode 430\tAvg Score: 0.0171, dqmin: 0.0000, dqmax: 0.1900, DQ: 0.0575/0.0569, pk: 74, mxpk: 38\n",
      "Mean score: 0.0575\t, old best: 0.0569, max_episode: 65\n",
      "\n",
      "Episode 440\tAvg Score: 0.0182, dqmin: 0.0000, dqmax: 0.1900, DQ: 0.0573/0.0575, pk: 81, mxpk: 38\n",
      "Episode 450\tAvg Score: 0.0191, dqmin: 0.0000, dqmax: 0.1900, DQ: 0.0602/0.0575, pk: 87, mxpk: 38\n",
      "Mean score: 0.0602\t, old best: 0.0575, max_episode: 65\n",
      "\n",
      "Episode 460\tAvg Score: 0.0205, dqmin: 0.0000, dqmax: 0.1900, DQ: 0.0627/0.0602, pk: 96, mxpk: 38\n",
      "Mean score: 0.0627\t, old best: 0.0602, max_episode: 65\n",
      "\n",
      "Episode 470\tAvg Score: 0.0222, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0726/0.0627, pk: 104, mxpk: 39\n",
      "Mean score: 0.0726\t, old best: 0.0627, max_episode: 130\n",
      "\n",
      "Episode 480\tAvg Score: 0.0219, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0687/0.0726, pk: 105, mxpk: 39\n",
      "Episode 490\tAvg Score: 0.0221, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0640/0.0726, pk: 107, mxpk: 39\n",
      "Episode 500\tAvg Score: 0.0222, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0590/0.0726, pk: 109, mxpk: 39\n",
      "Episode 510\tAvg Score: 0.0220, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0523/0.0726, pk: 110, mxpk: 39\n",
      "Episode 520\tAvg Score: 0.0217, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0463/0.0726, pk: 111, mxpk: 39\n",
      "Episode 530\tAvg Score: 0.0213, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0397/0.0726, pk: 111, mxpk: 39\n",
      "Episode 540\tAvg Score: 0.0215, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0358/0.0726, pk: 114, mxpk: 39\n",
      "Episode 550\tAvg Score: 0.0213, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0309/0.0726, pk: 115, mxpk: 39\n",
      "Episode 560\tAvg Score: 0.0214, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0251/0.0726, pk: 118, mxpk: 39\n",
      "Episode 570\tAvg Score: 0.0213, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0171/0.0726, pk: 120, mxpk: 39\n",
      "Episode 580\tAvg Score: 0.0209, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0162/0.0726, pk: 120, mxpk: 39\n",
      "Episode 590\tAvg Score: 0.0209, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0153/0.0726, pk: 122, mxpk: 39\n",
      "Episode 600\tAvg Score: 0.0209, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0144/0.0726, pk: 124, mxpk: 39\n",
      "Episode 610\tAvg Score: 0.0212, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0174/0.0726, pk: 128, mxpk: 39\n",
      "Episode 620\tAvg Score: 0.0216, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0210/0.0726, pk: 133, mxpk: 39\n",
      "Episode 630\tAvg Score: 0.0227, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0298/0.0726, pk: 142, mxpk: 39\n",
      "Episode 640\tAvg Score: 0.0232, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0328/0.0726, pk: 148, mxpk: 39\n",
      "Episode 650\tAvg Score: 0.0242, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0405/0.0726, pk: 157, mxpk: 39\n",
      "Episode 660\tAvg Score: 0.0247, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0436/0.0726, pk: 163, mxpk: 39\n",
      "Episode 670\tAvg Score: 0.0258, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0515/0.0726, pk: 173, mxpk: 39\n",
      "Episode 680\tAvg Score: 0.0267, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0600/0.0726, pk: 182, mxpk: 39\n",
      "Episode 690\tAvg Score: 0.0277, dqmin: 0.0000, dqmax: 0.1900, DQ: 0.0677/0.0726, pk: 191, mxpk: 39\n",
      "Episode 700\tAvg Score: 0.0288, dqmin: 0.0000, dqmax: 0.1900, DQ: 0.0765/0.0726, pk: 201, mxpk: 39\n",
      "Mean score: 0.0765\t, old best: 0.0726, max_episode: 130\n",
      "\n",
      "Episode 710\tAvg Score: 0.0294, dqmin: 0.0000, dqmax: 0.1900, DQ: 0.0791/0.0765, pk: 208, mxpk: 39\n",
      "Mean score: 0.0791\t, old best: 0.0765, max_episode: 130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 720\tAvg Score: 0.0300, dqmin: 0.0000, dqmax: 0.1900, DQ: 0.0823/0.0791, pk: 216, mxpk: 39\n",
      "Mean score: 0.0823\t, old best: 0.0791, max_episode: 130\n",
      "\n",
      "Episode 730\tAvg Score: 0.0307, dqmin: 0.0000, dqmax: 0.1900, DQ: 0.0813/0.0823, pk: 224, mxpk: 39\n",
      "Episode 740\tAvg Score: 0.0309, dqmin: 0.0000, dqmax: 0.1900, DQ: 0.0802/0.0823, pk: 229, mxpk: 39\n",
      "Episode 750\tAvg Score: 0.0315, dqmin: 0.0000, dqmax: 0.1900, DQ: 0.0792/0.0823, pk: 237, mxpk: 39\n",
      "Episode 760\tAvg Score: 0.0321, dqmin: 0.0000, dqmax: 0.1900, DQ: 0.0812/0.0823, pk: 245, mxpk: 39\n",
      "Episode 770\tAvg Score: 0.0327, dqmin: 0.0000, dqmax: 0.1900, DQ: 0.0788/0.0823, pk: 253, mxpk: 39\n",
      "Episode 780\tAvg Score: 0.0335, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0798/0.0823, pk: 262, mxpk: 39\n",
      "Episode 790\tAvg Score: 0.0342, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0788/0.0823, pk: 271, mxpk: 39\n",
      "Episode 800\tAvg Score: 0.0348, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0768/0.0823, pk: 280, mxpk: 39\n",
      "Episode 810\tAvg Score: 0.0354, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0781/0.0823, pk: 287, mxpk: 39\n",
      "Episode 820\tAvg Score: 0.0354, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0740/0.0823, pk: 291, mxpk: 39\n",
      "Episode 830\tAvg Score: 0.0359, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0739/0.0823, pk: 299, mxpk: 39\n",
      "Episode 840\tAvg Score: 0.0365, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0777/0.0823, pk: 308, mxpk: 39\n",
      "Episode 850\tAvg Score: 0.0369, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0776/0.0823, pk: 316, mxpk: 39\n",
      "Episode 860\tAvg Score: 0.0375, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0784/0.0823, pk: 325, mxpk: 39\n",
      "Episode 870\tAvg Score: 0.0375, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0747/0.0823, pk: 329, mxpk: 39\n",
      "Episode 880\tAvg Score: 0.0375, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0690/0.0823, pk: 333, mxpk: 39\n",
      "Episode 890\tAvg Score: 0.0379, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0671/0.0823, pk: 340, mxpk: 39\n",
      "Episode 900\tAvg Score: 0.0384, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0669/0.0823, pk: 349, mxpk: 39\n",
      "Episode 910\tAvg Score: 0.0387, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0660/0.0823, pk: 356, mxpk: 39\n",
      "Episode 920\tAvg Score: 0.0387, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0660/0.0823, pk: 360, mxpk: 39\n",
      "Episode 930\tAvg Score: 0.0388, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0633/0.0823, pk: 365, mxpk: 39\n",
      "Episode 940\tAvg Score: 0.0390, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0603/0.0823, pk: 371, mxpk: 39\n",
      "Episode 950\tAvg Score: 0.0391, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0575/0.0823, pk: 376, mxpk: 39\n",
      "Episode 960\tAvg Score: 0.0388, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0498/0.0823, pk: 377, mxpk: 39\n",
      "Episode 970\tAvg Score: 0.0392, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0536/0.0823, pk: 385, mxpk: 39\n",
      "Episode 980\tAvg Score: 0.0393, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0554/0.0823, pk: 391, mxpk: 39\n",
      "Episode 990\tAvg Score: 0.0394, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0534/0.0823, pk: 396, mxpk: 39\n",
      "Episode 1000\tAvg Score: 0.0393, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0478/0.0823, pk: 399, mxpk: 39\n",
      "Episode 1010\tAvg Score: 0.0394, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0459/0.0823, pk: 404, mxpk: 39\n",
      "Episode 1020\tAvg Score: 0.0395, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0472/0.0823, pk: 409, mxpk: 39\n",
      "Episode 1030\tAvg Score: 0.0396, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0469/0.0823, pk: 414, mxpk: 39\n",
      "Episode 1040\tAvg Score: 0.0399, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0480/0.0823, pk: 421, mxpk: 39\n",
      "Episode 1050\tAvg Score: 0.0400, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0488/0.0823, pk: 427, mxpk: 39\n",
      "Episode 1060\tAvg Score: 0.0402, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0534/0.0823, pk: 433, mxpk: 39\n",
      "Episode 1070\tAvg Score: 0.0403, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0508/0.0823, pk: 438, mxpk: 39\n",
      "Episode 1080\tAvg Score: 0.0403, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0502/0.0823, pk: 443, mxpk: 39\n",
      "Episode 1090\tAvg Score: 0.0405, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0511/0.0823, pk: 449, mxpk: 39\n",
      "Episode 1100\tAvg Score: 0.0407, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0547/0.0823, pk: 456, mxpk: 39\n",
      "Episode 1110\tAvg Score: 0.0408, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0545/0.0823, pk: 461, mxpk: 39\n",
      "Episode 1120\tAvg Score: 0.0410, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0563/0.0823, pk: 468, mxpk: 39\n",
      "Episode 1130\tAvg Score: 0.0411, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0564/0.0823, pk: 473, mxpk: 39\n",
      "Episode 1140\tAvg Score: 0.0411, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0536/0.0823, pk: 477, mxpk: 39\n",
      "Episode 1150\tAvg Score: 0.0412, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0538/0.0823, pk: 483, mxpk: 39\n",
      "Episode 1160\tAvg Score: 0.0415, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0552/0.0823, pk: 490, mxpk: 39\n",
      "Episode 1170\tAvg Score: 0.0417, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0572/0.0823, pk: 497, mxpk: 39\n",
      "Episode 1180\tAvg Score: 0.0416, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0551/0.0823, pk: 500, mxpk: 39\n",
      "Episode 1190\tAvg Score: 0.0417, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0544/0.0823, pk: 505, mxpk: 39\n",
      "Episode 1200\tAvg Score: 0.0417, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0526/0.0823, pk: 510, mxpk: 39\n",
      "Episode 1210\tAvg Score: 0.0417, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0518/0.0823, pk: 514, mxpk: 39\n",
      "Episode 1220\tAvg Score: 0.0421, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0540/0.0823, pk: 523, mxpk: 39\n",
      "Episode 1230\tAvg Score: 0.0422, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0551/0.0823, pk: 528, mxpk: 39\n",
      "Episode 1240\tAvg Score: 0.0422, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0552/0.0823, pk: 532, mxpk: 39\n",
      "Episode 1250\tAvg Score: 0.0424, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0564/0.0823, pk: 538, mxpk: 39\n",
      "Episode 1260\tAvg Score: 0.0427, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0565/0.0823, pk: 544, mxpk: 39\n",
      "Episode 1270\tAvg Score: 0.0429, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0566/0.0823, pk: 551, mxpk: 39\n",
      "Episode 1280\tAvg Score: 0.0429, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0586/0.0823, pk: 556, mxpk: 39\n",
      "Episode 1290\tAvg Score: 0.0428, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0565/0.0823, pk: 559, mxpk: 39\n",
      "Episode 1300\tAvg Score: 0.0429, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0566/0.0823, pk: 564, mxpk: 39\n",
      "Episode 1310\tAvg Score: 0.0431, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0606/0.0823, pk: 571, mxpk: 39\n",
      "Episode 1320\tAvg Score: 0.0433, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0586/0.0823, pk: 576, mxpk: 39\n",
      "Episode 1330\tAvg Score: 0.0434, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0577/0.0823, pk: 581, mxpk: 39\n",
      "Episode 1340\tAvg Score: 0.0434, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0577/0.0823, pk: 585, mxpk: 39\n",
      "Episode 1350\tAvg Score: 0.0436, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0576/0.0823, pk: 591, mxpk: 39\n",
      "Episode 1360\tAvg Score: 0.0435, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0545/0.0823, pk: 595, mxpk: 39\n",
      "Episode 1370\tAvg Score: 0.0437, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0544/0.0823, pk: 601, mxpk: 39\n",
      "Episode 1380\tAvg Score: 0.0437, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0534/0.0823, pk: 605, mxpk: 39\n",
      "Episode 1390\tAvg Score: 0.0438, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0563/0.0823, pk: 610, mxpk: 39\n",
      "Episode 1400\tAvg Score: 0.0437, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0543/0.0823, pk: 613, mxpk: 39\n",
      "Episode 1410\tAvg Score: 0.0436, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0501/0.0823, pk: 617, mxpk: 39\n",
      "Episode 1420\tAvg Score: 0.0438, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0501/0.0823, pk: 624, mxpk: 39\n",
      "Episode 1430\tAvg Score: 0.0438, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0490/0.0823, pk: 628, mxpk: 39\n",
      "Episode 1440\tAvg Score: 0.0441, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0539/0.0823, pk: 637, mxpk: 39\n",
      "Episode 1450\tAvg Score: 0.0442, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0525/0.0823, pk: 643, mxpk: 39\n",
      "Episode 1460\tAvg Score: 0.0441, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0514/0.0823, pk: 646, mxpk: 39\n",
      "Episode 1470\tAvg Score: 0.0439, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0464/0.0823, pk: 648, mxpk: 39\n",
      "Episode 1480\tAvg Score: 0.0437, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0445/0.0823, pk: 650, mxpk: 39\n",
      "Episode 1490\tAvg Score: 0.0436, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0406/0.0823, pk: 652, mxpk: 39\n",
      "Episode 1500\tAvg Score: 0.0437, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0435/0.0823, pk: 657, mxpk: 39\n",
      "Episode 1510\tAvg Score: 0.0438, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0457/0.0823, pk: 662, mxpk: 39\n",
      "Episode 1520\tAvg Score: 0.0439, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0445/0.0823, pk: 668, mxpk: 39\n",
      "Episode 1530\tAvg Score: 0.0438, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0445/0.0823, pk: 671, mxpk: 39\n",
      "Episode 1540\tAvg Score: 0.0439, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0403/0.0823, pk: 676, mxpk: 39\n",
      "Episode 1550\tAvg Score: 0.0439, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0396/0.0823, pk: 680, mxpk: 39\n",
      "Episode 1560\tAvg Score: 0.0438, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0406/0.0823, pk: 684, mxpk: 39\n",
      "Episode 1570\tAvg Score: 0.0437, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0416/0.0823, pk: 687, mxpk: 39\n",
      "Episode 1580\tAvg Score: 0.0435, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0405/0.0823, pk: 688, mxpk: 39\n",
      "Episode 1590\tAvg Score: 0.0435, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0425/0.0823, pk: 692, mxpk: 39\n",
      "Episode 1600\tAvg Score: 0.0435, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0406/0.0823, pk: 696, mxpk: 39\n",
      "Episode 1610\tAvg Score: 0.0434, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0385/0.0823, pk: 699, mxpk: 39\n",
      "Episode 1620\tAvg Score: 0.0435, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0383/0.0823, pk: 705, mxpk: 39\n",
      "Episode 1630\tAvg Score: 0.0434, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0372/0.0823, pk: 708, mxpk: 39\n",
      "Episode 1640\tAvg Score: 0.0434, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0363/0.0823, pk: 711, mxpk: 39\n",
      "Episode 1650\tAvg Score: 0.0433, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0343/0.0823, pk: 714, mxpk: 39\n",
      "Episode 1660\tAvg Score: 0.0433, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0343/0.0823, pk: 718, mxpk: 39\n",
      "Episode 1670\tAvg Score: 0.0432, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0342/0.0823, pk: 721, mxpk: 39\n",
      "Episode 1680\tAvg Score: 0.0431, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0371/0.0823, pk: 725, mxpk: 39\n",
      "Episode 1690\tAvg Score: 0.0432, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0391/0.0823, pk: 731, mxpk: 39\n",
      "Episode 1700\tAvg Score: 0.0430, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0352/0.0823, pk: 731, mxpk: 39\n",
      "Episode 1710\tAvg Score: 0.0430, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0362/0.0823, pk: 736, mxpk: 39\n",
      "Episode 1720\tAvg Score: 0.0429, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0334/0.0823, pk: 739, mxpk: 39\n",
      "Episode 1730\tAvg Score: 0.0428, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0325/0.0823, pk: 741, mxpk: 39\n",
      "Episode 1740\tAvg Score: 0.0428, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0324/0.0823, pk: 745, mxpk: 39\n",
      "Episode 1750\tAvg Score: 0.0426, dqmin: 0.0000, dqmax: 0.1000, DQ: 0.0305/0.0823, pk: 746, mxpk: 39\n",
      "Episode 1760\tAvg Score: 0.0425, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0306/0.0823, pk: 749, mxpk: 39\n",
      "Episode 1770\tAvg Score: 0.0426, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0325/0.0823, pk: 754, mxpk: 39\n",
      "Episode 1780\tAvg Score: 0.0426, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0335/0.0823, pk: 759, mxpk: 39\n",
      "Episode 1790\tAvg Score: 0.0427, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0342/0.0823, pk: 766, mxpk: 39\n",
      "Episode 1800\tAvg Score: 0.0428, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0389/0.0823, pk: 771, mxpk: 39\n",
      "Episode 1810\tAvg Score: 0.0428, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0388/0.0823, pk: 776, mxpk: 39\n",
      "Episode 1820\tAvg Score: 0.0430, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0434/0.0823, pk: 784, mxpk: 39\n",
      "Episode 1830\tAvg Score: 0.0432, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0501/0.0823, pk: 793, mxpk: 39\n",
      "Episode 1840\tAvg Score: 0.0433, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0531/0.0823, pk: 800, mxpk: 39\n",
      "Episode 1850\tAvg Score: 0.0435, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0598/0.0823, pk: 808, mxpk: 39\n",
      "Episode 1860\tAvg Score: 0.0438, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0657/0.0823, pk: 817, mxpk: 39\n",
      "Episode 1870\tAvg Score: 0.0439, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0675/0.0823, pk: 824, mxpk: 39\n",
      "Episode 1880\tAvg Score: 0.0440, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0696/0.0823, pk: 831, mxpk: 39\n",
      "Episode 1890\tAvg Score: 0.0443, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0715/0.0823, pk: 840, mxpk: 39\n",
      "Episode 1900\tAvg Score: 0.0442, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0705/0.0823, pk: 844, mxpk: 39\n",
      "Episode 1910\tAvg Score: 0.0444, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0735/0.0823, pk: 851, mxpk: 39\n",
      "Episode 1920\tAvg Score: 0.0446, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0749/0.0823, pk: 860, mxpk: 39\n",
      "Episode 1930\tAvg Score: 0.0448, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0739/0.0823, pk: 868, mxpk: 39\n",
      "Episode 1940\tAvg Score: 0.0450, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0765/0.0823, pk: 878, mxpk: 39\n",
      "Episode 1950\tAvg Score: 0.0451, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0747/0.0823, pk: 884, mxpk: 39\n",
      "Episode 1960\tAvg Score: 0.0452, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0705/0.0823, pk: 890, mxpk: 39\n",
      "Episode 1970\tAvg Score: 0.0454, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0736/0.0823, pk: 899, mxpk: 39\n",
      "Episode 1980\tAvg Score: 0.0457, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0764/0.0823, pk: 907, mxpk: 39\n",
      "Episode 1990\tAvg Score: 0.0458, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0754/0.0823, pk: 915, mxpk: 39\n",
      "Episode 2000\tAvg Score: 0.0460, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0795/0.0823, pk: 922, mxpk: 39\n",
      "Episode 2010\tAvg Score: 0.0463, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0836/0.0823, pk: 932, mxpk: 39\n",
      "Mean score: 0.0836\t, old best: 0.0823, max_episode: 130\n",
      "\n",
      "Episode 2020\tAvg Score: 0.0466, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0854/0.0836, pk: 942, mxpk: 39\n",
      "Mean score: 0.0854\t, old best: 0.0836, max_episode: 130\n",
      "\n",
      "Episode 2030\tAvg Score: 0.0468, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0867/0.0854, pk: 947, mxpk: 40\n",
      "Mean score: 0.0867\t, old best: 0.0854, max_episode: 130\n",
      "\n",
      "Episode 2040\tAvg Score: 0.0471, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0880/0.0867, pk: 956, mxpk: 40\n",
      "Mean score: 0.0880\t, old best: 0.0867, max_episode: 130\n",
      "\n",
      "Episode 2050\tAvg Score: 0.0474, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0928/0.0880, pk: 966, mxpk: 40\n",
      "Mean score: 0.0928\t, old best: 0.0880, max_episode: 130\n",
      "\n",
      "Episode 2060\tAvg Score: 0.0477, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0981/0.0928, pk: 973, mxpk: 41\n",
      "Mean score: 0.0981\t, old best: 0.0928, max_episode: 130\n",
      "\n",
      "Episode 2070\tAvg Score: 0.0478, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0951/0.0981, pk: 980, mxpk: 41\n",
      "Episode 2080\tAvg Score: 0.0479, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0918/0.0981, pk: 987, mxpk: 41\n",
      "Episode 2090\tAvg Score: 0.0479, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0899/0.0981, pk: 993, mxpk: 41\n",
      "Episode 2100\tAvg Score: 0.0480, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0887/0.0981, pk: 998, mxpk: 42\n",
      "Episode 2110\tAvg Score: 0.0480, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0806/0.0981, pk: 1001, mxpk: 42\n",
      "Episode 2120\tAvg Score: 0.0482, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0799/0.0981, pk: 1007, mxpk: 43\n",
      "Episode 2130\tAvg Score: 0.0482, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0748/0.0981, pk: 1011, mxpk: 43\n",
      "Episode 2140\tAvg Score: 0.0480, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0661/0.0981, pk: 1013, mxpk: 43\n",
      "Episode 2150\tAvg Score: 0.0480, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0593/0.0981, pk: 1017, mxpk: 43\n",
      "Episode 2160\tAvg Score: 0.0480, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0532/0.0981, pk: 1021, mxpk: 43\n",
      "Episode 2170\tAvg Score: 0.0480, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0516/0.0981, pk: 1026, mxpk: 43\n",
      "Episode 2180\tAvg Score: 0.0479, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0471/0.0981, pk: 1028, mxpk: 43\n",
      "Episode 2190\tAvg Score: 0.0478, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0445/0.0981, pk: 1030, mxpk: 43\n",
      "Episode 2200\tAvg Score: 0.0476, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0389/0.0981, pk: 1031, mxpk: 43\n",
      "Episode 2210\tAvg Score: 0.0475, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0369/0.0981, pk: 1033, mxpk: 43\n",
      "Episode 2220\tAvg Score: 0.0473, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0288/0.0981, pk: 1035, mxpk: 43\n",
      "Episode 2230\tAvg Score: 0.0473, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0289/0.0981, pk: 1039, mxpk: 43\n",
      "Episode 2240\tAvg Score: 0.0473, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0320/0.0981, pk: 1042, mxpk: 43\n",
      "Episode 2250\tAvg Score: 0.0472, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0301/0.0981, pk: 1044, mxpk: 43\n",
      "Episode 2260\tAvg Score: 0.0471, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0281/0.0981, pk: 1046, mxpk: 43\n",
      "Episode 2270\tAvg Score: 0.0471, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0289/0.0981, pk: 1051, mxpk: 43\n",
      "Episode 2280\tAvg Score: 0.0471, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0300/0.0981, pk: 1054, mxpk: 43\n",
      "Episode 2290\tAvg Score: 0.0471, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0317/0.0981, pk: 1059, mxpk: 43\n",
      "Episode 2300\tAvg Score: 0.0472, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0385/0.0981, pk: 1066, mxpk: 43\n",
      "Episode 2310\tAvg Score: 0.0472, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0415/0.0981, pk: 1070, mxpk: 43\n",
      "Episode 2320\tAvg Score: 0.0472, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0434/0.0981, pk: 1074, mxpk: 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2330\tAvg Score: 0.0472, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0453/0.0981, pk: 1079, mxpk: 43\n",
      "Episode 2340\tAvg Score: 0.0472, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0459/0.0981, pk: 1084, mxpk: 43\n",
      "Episode 2350\tAvg Score: 0.0472, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0479/0.0981, pk: 1087, mxpk: 43\n",
      "Episode 2360\tAvg Score: 0.0473, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0507/0.0981, pk: 1093, mxpk: 43\n",
      "Episode 2370\tAvg Score: 0.0473, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0513/0.0981, pk: 1100, mxpk: 43\n",
      "Episode 2380\tAvg Score: 0.0473, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0531/0.0981, pk: 1104, mxpk: 43\n",
      "Episode 2390\tAvg Score: 0.0473, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0523/0.0981, pk: 1108, mxpk: 43\n",
      "Episode 2400\tAvg Score: 0.0472, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0474/0.0981, pk: 1111, mxpk: 43\n",
      "Episode 2410\tAvg Score: 0.0473, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0503/0.0981, pk: 1117, mxpk: 44\n",
      "Episode 2420\tAvg Score: 0.0474, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0525/0.0981, pk: 1122, mxpk: 44\n",
      "Episode 2430\tAvg Score: 0.0474, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0525/0.0981, pk: 1128, mxpk: 44\n",
      "Episode 2440\tAvg Score: 0.0475, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0527/0.0981, pk: 1134, mxpk: 44\n",
      "Episode 2450\tAvg Score: 0.0476, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0555/0.0981, pk: 1140, mxpk: 44\n",
      "Episode 2460\tAvg Score: 0.0475, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0545/0.0981, pk: 1145, mxpk: 44\n",
      "Episode 2470\tAvg Score: 0.0475, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0518/0.0981, pk: 1149, mxpk: 44\n",
      "Episode 2480\tAvg Score: 0.0475, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0507/0.0981, pk: 1153, mxpk: 44\n",
      "Episode 2490\tAvg Score: 0.0475, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0526/0.0981, pk: 1158, mxpk: 44\n",
      "Episode 2500\tAvg Score: 0.0475, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0552/0.0981, pk: 1164, mxpk: 44\n",
      "Episode 2510\tAvg Score: 0.0477, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0552/0.0981, pk: 1172, mxpk: 44\n",
      "Episode 2520\tAvg Score: 0.0477, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0548/0.0981, pk: 1178, mxpk: 44\n",
      "Episode 2530\tAvg Score: 0.0476, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0528/0.0981, pk: 1182, mxpk: 44\n",
      "Episode 2540\tAvg Score: 0.0476, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0517/0.0981, pk: 1187, mxpk: 44\n",
      "Episode 2550\tAvg Score: 0.0479, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0556/0.0981, pk: 1197, mxpk: 44\n",
      "Episode 2560\tAvg Score: 0.0479, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0556/0.0981, pk: 1201, mxpk: 44\n",
      "Episode 2570\tAvg Score: 0.0479, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0585/0.0981, pk: 1208, mxpk: 44\n",
      "Episode 2580\tAvg Score: 0.0481, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0636/0.0981, pk: 1216, mxpk: 44\n",
      "Episode 2590\tAvg Score: 0.0482, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0662/0.0981, pk: 1225, mxpk: 44\n",
      "Episode 2600\tAvg Score: 0.0482, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0655/0.0981, pk: 1229, mxpk: 44\n",
      "Episode 2610\tAvg Score: 0.0483, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0637/0.0981, pk: 1234, mxpk: 44\n",
      "Episode 2620\tAvg Score: 0.0483, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0640/0.0981, pk: 1239, mxpk: 44\n",
      "Episode 2630\tAvg Score: 0.0484, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0682/0.0981, pk: 1245, mxpk: 44\n",
      "Episode 2640\tAvg Score: 0.0486, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0722/0.0981, pk: 1252, mxpk: 44\n",
      "Episode 2650\tAvg Score: 0.0487, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0691/0.0981, pk: 1260, mxpk: 44\n",
      "Episode 2660\tAvg Score: 0.0487, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0712/0.0981, pk: 1266, mxpk: 44\n",
      "Episode 2670\tAvg Score: 0.0489, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0740/0.0981, pk: 1275, mxpk: 44\n",
      "Episode 2680\tAvg Score: 0.0490, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0730/0.0981, pk: 1281, mxpk: 44\n",
      "Episode 2690\tAvg Score: 0.0491, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0724/0.0981, pk: 1287, mxpk: 45\n",
      "Episode 2700\tAvg Score: 0.0492, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0753/0.0981, pk: 1295, mxpk: 45\n",
      "Episode 2710\tAvg Score: 0.0493, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0770/0.0981, pk: 1302, mxpk: 45\n",
      "Episode 2720\tAvg Score: 0.0493, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0751/0.0981, pk: 1306, mxpk: 45\n",
      "Episode 2730\tAvg Score: 0.0494, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0758/0.0981, pk: 1313, mxpk: 45\n",
      "Episode 2740\tAvg Score: 0.0495, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0749/0.0981, pk: 1321, mxpk: 45\n",
      "Episode 2750\tAvg Score: 0.0495, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0724/0.0981, pk: 1324, mxpk: 45\n",
      "Episode 2760\tAvg Score: 0.0497, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0763/0.0981, pk: 1333, mxpk: 46\n",
      "Episode 2770\tAvg Score: 0.0498, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0725/0.0981, pk: 1339, mxpk: 46\n",
      "Episode 2780\tAvg Score: 0.0499, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0735/0.0981, pk: 1347, mxpk: 46\n",
      "Episode 2790\tAvg Score: 0.0501, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0755/0.0981, pk: 1354, mxpk: 46\n",
      "Episode 2800\tAvg Score: 0.0501, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0744/0.0981, pk: 1361, mxpk: 46\n",
      "Episode 2810\tAvg Score: 0.0502, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0735/0.0981, pk: 1367, mxpk: 46\n",
      "Episode 2820\tAvg Score: 0.0504, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0793/0.0981, pk: 1376, mxpk: 46\n",
      "Episode 2830\tAvg Score: 0.0504, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0773/0.0981, pk: 1381, mxpk: 46\n",
      "Episode 2840\tAvg Score: 0.0504, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0743/0.0981, pk: 1386, mxpk: 46\n",
      "Episode 2850\tAvg Score: 0.0505, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0781/0.0981, pk: 1394, mxpk: 46\n",
      "Episode 2860\tAvg Score: 0.0506, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0741/0.0981, pk: 1401, mxpk: 46\n",
      "Episode 2870\tAvg Score: 0.0505, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0724/0.0981, pk: 1405, mxpk: 46\n",
      "Episode 2880\tAvg Score: 0.0507, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0735/0.0981, pk: 1412, mxpk: 46\n",
      "Episode 2890\tAvg Score: 0.0507, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0695/0.0981, pk: 1417, mxpk: 46\n",
      "Episode 2900\tAvg Score: 0.0508, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0705/0.0981, pk: 1424, mxpk: 46\n",
      "Episode 2910\tAvg Score: 0.0510, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0735/0.0981, pk: 1434, mxpk: 46\n",
      "Episode 2920\tAvg Score: 0.0511, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0708/0.0981, pk: 1440, mxpk: 46\n",
      "Episode 2930\tAvg Score: 0.0512, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0736/0.0981, pk: 1450, mxpk: 46\n",
      "Episode 2940\tAvg Score: 0.0513, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0756/0.0981, pk: 1457, mxpk: 46\n",
      "Episode 2950\tAvg Score: 0.0514, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0756/0.0981, pk: 1465, mxpk: 46\n",
      "Episode 2960\tAvg Score: 0.0515, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0767/0.0981, pk: 1471, mxpk: 47\n",
      "Episode 2970\tAvg Score: 0.0515, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0776/0.0981, pk: 1476, mxpk: 47\n",
      "Episode 2980\tAvg Score: 0.0515, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0733/0.0981, pk: 1482, mxpk: 47\n",
      "Episode 2990\tAvg Score: 0.0516, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0773/0.0981, pk: 1490, mxpk: 47\n",
      "Episode 3000\tAvg Score: 0.0517, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0772/0.0981, pk: 1498, mxpk: 47\n",
      "Episode 3010\tAvg Score: 0.0518, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0762/0.0981, pk: 1506, mxpk: 47\n",
      "Episode 3020\tAvg Score: 0.0519, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0759/0.0981, pk: 1513, mxpk: 47\n",
      "Episode 3030\tAvg Score: 0.0519, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0712/0.0981, pk: 1518, mxpk: 47\n",
      "Episode 3040\tAvg Score: 0.0520, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0723/0.0981, pk: 1524, mxpk: 48\n",
      "Episode 3050\tAvg Score: 0.0521, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0720/0.0981, pk: 1531, mxpk: 48\n",
      "Episode 3060\tAvg Score: 0.0522, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0727/0.0981, pk: 1540, mxpk: 48\n",
      "Episode 3070\tAvg Score: 0.0523, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0767/0.0981, pk: 1548, mxpk: 48\n",
      "Episode 3080\tAvg Score: 0.0524, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0809/0.0981, pk: 1557, mxpk: 48\n",
      "Episode 3090\tAvg Score: 0.0524, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0770/0.0981, pk: 1563, mxpk: 48\n",
      "Episode 3100\tAvg Score: 0.0526, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0782/0.0981, pk: 1572, mxpk: 48\n",
      "Episode 3110\tAvg Score: 0.0527, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0792/0.0981, pk: 1582, mxpk: 48\n",
      "Episode 3120\tAvg Score: 0.0528, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0815/0.0981, pk: 1589, mxpk: 48\n",
      "Episode 3130\tAvg Score: 0.0529, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0846/0.0981, pk: 1596, mxpk: 48\n",
      "Episode 3140\tAvg Score: 0.0530, dqmin: 0.0000, dqmax: 0.2900, DQ: 0.0834/0.0981, pk: 1603, mxpk: 48\n",
      "Episode 3150\tAvg Score: 0.0531, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0855/0.0981, pk: 1611, mxpk: 49\n",
      "Episode 3160\tAvg Score: 0.0532, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0839/0.0981, pk: 1617, mxpk: 49\n",
      "Episode 3170\tAvg Score: 0.0532, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0801/0.0981, pk: 1622, mxpk: 49\n",
      "Episode 3180\tAvg Score: 0.0531, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0751/0.0981, pk: 1627, mxpk: 49\n",
      "Episode 3190\tAvg Score: 0.0533, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0790/0.0981, pk: 1636, mxpk: 49\n",
      "Episode 3200\tAvg Score: 0.0534, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0799/0.0981, pk: 1644, mxpk: 50\n",
      "Episode 3210\tAvg Score: 0.0534, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0762/0.0981, pk: 1650, mxpk: 50\n",
      "Episode 3220\tAvg Score: 0.0536, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0771/0.0981, pk: 1659, mxpk: 50\n",
      "Episode 3230\tAvg Score: 0.0536, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0769/0.0981, pk: 1666, mxpk: 50\n",
      "Episode 3240\tAvg Score: 0.0538, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0789/0.0981, pk: 1674, mxpk: 50\n",
      "Episode 3250\tAvg Score: 0.0539, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0771/0.0981, pk: 1683, mxpk: 50\n",
      "Episode 3260\tAvg Score: 0.0540, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0801/0.0981, pk: 1692, mxpk: 50\n",
      "Episode 3270\tAvg Score: 0.0541, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0830/0.0981, pk: 1700, mxpk: 50\n",
      "Episode 3280\tAvg Score: 0.0541, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0851/0.0981, pk: 1707, mxpk: 50\n",
      "Episode 3290\tAvg Score: 0.0543, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.0861/0.0981, pk: 1716, mxpk: 50\n",
      "Episode 3300\tAvg Score: 0.0543, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0832/0.0981, pk: 1722, mxpk: 50\n",
      "Episode 3310\tAvg Score: 0.0544, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0857/0.0981, pk: 1731, mxpk: 50\n",
      "Episode 3320\tAvg Score: 0.0545, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0847/0.0981, pk: 1740, mxpk: 50\n",
      "Episode 3330\tAvg Score: 0.0545, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0828/0.0981, pk: 1746, mxpk: 50\n",
      "Episode 3340\tAvg Score: 0.0547, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0869/0.0981, pk: 1756, mxpk: 50\n",
      "Episode 3350\tAvg Score: 0.0549, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0879/0.0981, pk: 1765, mxpk: 50\n",
      "Episode 3360\tAvg Score: 0.0550, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0889/0.0981, pk: 1774, mxpk: 50\n",
      "Episode 3370\tAvg Score: 0.0552, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0927/0.0981, pk: 1784, mxpk: 50\n",
      "Episode 3380\tAvg Score: 0.0554, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0974/0.0981, pk: 1794, mxpk: 50\n",
      "Episode 3390\tAvg Score: 0.0555, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.0973/0.0981, pk: 1804, mxpk: 50\n",
      "Episode 3400\tAvg Score: 0.0557, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.1003/0.0981, pk: 1814, mxpk: 50\n",
      "Mean score: 0.1003\t, old best: 0.0981, max_episode: 146\n",
      "\n",
      "Episode 3410\tAvg Score: 0.0558, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.1002/0.1003, pk: 1823, mxpk: 50\n",
      "Episode 3420\tAvg Score: 0.0559, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.1010/0.1003, pk: 1832, mxpk: 50\n",
      "Mean score: 0.1010\t, old best: 0.1003, max_episode: 146\n",
      "\n",
      "Episode 3430\tAvg Score: 0.0561, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.1069/0.1010, pk: 1842, mxpk: 50\n",
      "Mean score: 0.1069\t, old best: 0.1010, max_episode: 146\n",
      "\n",
      "Episode 3440\tAvg Score: 0.0562, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.1061/0.1069, pk: 1852, mxpk: 50\n",
      "Episode 3450\tAvg Score: 0.0564, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.1059/0.1069, pk: 1861, mxpk: 50\n",
      "Episode 3460\tAvg Score: 0.0564, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.1039/0.1069, pk: 1868, mxpk: 50\n",
      "Episode 3470\tAvg Score: 0.0566, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.1030/0.1069, pk: 1878, mxpk: 50\n",
      "Episode 3480\tAvg Score: 0.0567, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.1024/0.1069, pk: 1888, mxpk: 50\n",
      "Episode 3490\tAvg Score: 0.0569, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1033/0.1069, pk: 1897, mxpk: 51\n",
      "Episode 3500\tAvg Score: 0.0571, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1053/0.1069, pk: 1906, mxpk: 51\n",
      "Episode 3510\tAvg Score: 0.0573, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1086/0.1069, pk: 1915, mxpk: 51\n",
      "Mean score: 0.1086\t, old best: 0.1069, max_episode: 146\n",
      "\n",
      "Episode 3520\tAvg Score: 0.0574, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1096/0.1086, pk: 1925, mxpk: 51\n",
      "Mean score: 0.1096\t, old best: 0.1086, max_episode: 146\n",
      "\n",
      "Episode 3530\tAvg Score: 0.0575, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1072/0.1096, pk: 1935, mxpk: 51\n",
      "Episode 3540\tAvg Score: 0.0577, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1079/0.1096, pk: 1945, mxpk: 51\n",
      "Episode 3550\tAvg Score: 0.0578, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1080/0.1096, pk: 1954, mxpk: 51\n",
      "Episode 3560\tAvg Score: 0.0579, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1087/0.1096, pk: 1964, mxpk: 51\n",
      "Episode 3570\tAvg Score: 0.0580, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1085/0.1096, pk: 1974, mxpk: 51\n",
      "Episode 3580\tAvg Score: 0.0581, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1069/0.1096, pk: 1984, mxpk: 51\n",
      "Episode 3590\tAvg Score: 0.0583, dqmin: 0.0000, dqmax: 0.2900, DQ: 0.1060/0.1096, pk: 1994, mxpk: 51\n",
      "Episode 3600\tAvg Score: 0.0584, dqmin: 0.0000, dqmax: 0.2900, DQ: 0.1060/0.1096, pk: 2004, mxpk: 51\n",
      "Episode 3610\tAvg Score: 0.0587, dqmin: 0.0000, dqmax: 0.2900, DQ: 0.1079/0.1096, pk: 2014, mxpk: 51\n",
      "Episode 3620\tAvg Score: 0.0589, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1101/0.1096, pk: 2023, mxpk: 52\n",
      "Mean score: 0.1101\t, old best: 0.1096, max_episode: 146\n",
      "\n",
      "Episode 3630\tAvg Score: 0.0590, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1114/0.1101, pk: 2033, mxpk: 52\n",
      "Mean score: 0.1114\t, old best: 0.1101, max_episode: 146\n",
      "\n",
      "Episode 3640\tAvg Score: 0.0591, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1094/0.1114, pk: 2043, mxpk: 52\n",
      "Episode 3650\tAvg Score: 0.0593, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1116/0.1114, pk: 2053, mxpk: 52\n",
      "Mean score: 0.1116\t, old best: 0.1114, max_episode: 146\n",
      "\n",
      "Episode 3660\tAvg Score: 0.0594, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1130/0.1116, pk: 2063, mxpk: 52\n",
      "Mean score: 0.1130\t, old best: 0.1116, max_episode: 146\n",
      "\n",
      "Episode 3670\tAvg Score: 0.0596, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1138/0.1130, pk: 2073, mxpk: 52\n",
      "Mean score: 0.1138\t, old best: 0.1130, max_episode: 146\n",
      "\n",
      "Episode 3680\tAvg Score: 0.0598, dqmin: 0.0000, dqmax: 0.4000, DQ: 0.1191/0.1138, pk: 2083, mxpk: 53\n",
      "Mean score: 0.1191\t, old best: 0.1138, max_episode: 182\n",
      "\n",
      "Episode 3690\tAvg Score: 0.0600, dqmin: 0.0000, dqmax: 0.4000, DQ: 0.1203/0.1191, pk: 2093, mxpk: 53\n",
      "Mean score: 0.1203\t, old best: 0.1191, max_episode: 182\n",
      "\n",
      "Episode 3700\tAvg Score: 0.0602, dqmin: 0.0000, dqmax: 0.4000, DQ: 0.1225/0.1203, pk: 2103, mxpk: 53\n",
      "Mean score: 0.1225\t, old best: 0.1203, max_episode: 182\n",
      "\n",
      "Episode 3710\tAvg Score: 0.0603, dqmin: 0.0000, dqmax: 0.4000, DQ: 0.1185/0.1225, pk: 2113, mxpk: 53\n",
      "Episode 3720\tAvg Score: 0.0604, dqmin: 0.0900, dqmax: 0.4000, DQ: 0.1173/0.1225, pk: 2123, mxpk: 53\n",
      "Episode 3730\tAvg Score: 0.0606, dqmin: 0.0900, dqmax: 0.4000, DQ: 0.1186/0.1225, pk: 2133, mxpk: 53\n",
      "Episode 3740\tAvg Score: 0.0608, dqmin: 0.0900, dqmax: 0.4000, DQ: 0.1207/0.1225, pk: 2143, mxpk: 53\n",
      "Episode 3750\tAvg Score: 0.0609, dqmin: 0.0900, dqmax: 0.4000, DQ: 0.1216/0.1225, pk: 2153, mxpk: 53\n",
      "Episode 3760\tAvg Score: 0.0611, dqmin: 0.0900, dqmax: 0.4000, DQ: 0.1225/0.1225, pk: 2163, mxpk: 53\n",
      "Episode 3770\tAvg Score: 0.0613, dqmin: 0.0900, dqmax: 0.4000, DQ: 0.1239/0.1225, pk: 2173, mxpk: 53\n",
      "Mean score: 0.1239\t, old best: 0.1225, max_episode: 182\n",
      "\n",
      "Episode 3780\tAvg Score: 0.0614, dqmin: 0.0900, dqmax: 0.3000, DQ: 0.1219/0.1239, pk: 2183, mxpk: 53\n",
      "Episode 3790\tAvg Score: 0.0616, dqmin: 0.0900, dqmax: 0.3000, DQ: 0.1225/0.1239, pk: 2193, mxpk: 53\n",
      "Episode 3800\tAvg Score: 0.0617, dqmin: 0.0900, dqmax: 0.3000, DQ: 0.1192/0.1239, pk: 2203, mxpk: 53\n",
      "Episode 3810\tAvg Score: 0.0620, dqmin: 0.0900, dqmax: 0.3000, DQ: 0.1244/0.1239, pk: 2213, mxpk: 53\n",
      "Mean score: 0.1244\t, old best: 0.1239, max_episode: 182\n",
      "\n",
      "Episode 3820\tAvg Score: 0.0620, dqmin: 0.0900, dqmax: 0.3000, DQ: 0.1225/0.1244, pk: 2223, mxpk: 53\n",
      "Episode 3830\tAvg Score: 0.0622, dqmin: 0.0900, dqmax: 0.3000, DQ: 0.1221/0.1244, pk: 2233, mxpk: 53\n",
      "Episode 3840\tAvg Score: 0.0624, dqmin: 0.0900, dqmax: 0.3000, DQ: 0.1230/0.1244, pk: 2243, mxpk: 53\n",
      "Episode 3850\tAvg Score: 0.0625, dqmin: 0.0900, dqmax: 0.3000, DQ: 0.1211/0.1244, pk: 2253, mxpk: 53\n",
      "Episode 3860\tAvg Score: 0.0626, dqmin: 0.0900, dqmax: 0.3000, DQ: 0.1211/0.1244, pk: 2263, mxpk: 53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3870\tAvg Score: 0.0629, dqmin: 0.0900, dqmax: 0.3000, DQ: 0.1229/0.1244, pk: 2273, mxpk: 53\n",
      "Episode 3880\tAvg Score: 0.0630, dqmin: 0.0900, dqmax: 0.3000, DQ: 0.1219/0.1244, pk: 2283, mxpk: 53\n",
      "Episode 3890\tAvg Score: 0.0631, dqmin: 0.0900, dqmax: 0.3000, DQ: 0.1192/0.1244, pk: 2293, mxpk: 53\n",
      "Episode 3900\tAvg Score: 0.0632, dqmin: 0.0900, dqmax: 0.3000, DQ: 0.1204/0.1244, pk: 2303, mxpk: 53\n",
      "Episode 3910\tAvg Score: 0.0634, dqmin: 0.0900, dqmax: 0.3000, DQ: 0.1174/0.1244, pk: 2313, mxpk: 53\n",
      "Episode 3920\tAvg Score: 0.0635, dqmin: 0.0900, dqmax: 0.3000, DQ: 0.1176/0.1244, pk: 2323, mxpk: 53\n",
      "Episode 3930\tAvg Score: 0.0636, dqmin: 0.0900, dqmax: 0.3000, DQ: 0.1177/0.1244, pk: 2333, mxpk: 53\n",
      "Episode 3940\tAvg Score: 0.0637, dqmin: 0.0900, dqmax: 0.2000, DQ: 0.1144/0.1244, pk: 2343, mxpk: 53\n",
      "Episode 3950\tAvg Score: 0.0638, dqmin: 0.0900, dqmax: 0.2000, DQ: 0.1144/0.1244, pk: 2353, mxpk: 53\n",
      "Episode 3960\tAvg Score: 0.0639, dqmin: 0.0900, dqmax: 0.2000, DQ: 0.1133/0.1244, pk: 2363, mxpk: 53\n",
      "Episode 3970\tAvg Score: 0.0640, dqmin: 0.0900, dqmax: 0.2000, DQ: 0.1082/0.1244, pk: 2373, mxpk: 53\n",
      "Episode 3980\tAvg Score: 0.0641, dqmin: 0.0900, dqmax: 0.2000, DQ: 0.1072/0.1244, pk: 2383, mxpk: 53\n",
      "Episode 3990\tAvg Score: 0.0642, dqmin: 0.0900, dqmax: 0.2000, DQ: 0.1080/0.1244, pk: 2393, mxpk: 53\n",
      "Episode 4000\tAvg Score: 0.0643, dqmin: 0.0900, dqmax: 0.2000, DQ: 0.1079/0.1244, pk: 2403, mxpk: 53\n",
      "Episode 4010\tAvg Score: 0.0645, dqmin: 0.0900, dqmax: 0.2000, DQ: 0.1076/0.1244, pk: 2413, mxpk: 53\n",
      "Episode 4020\tAvg Score: 0.0645, dqmin: 0.0900, dqmax: 0.2000, DQ: 0.1069/0.1244, pk: 2423, mxpk: 53\n",
      "Episode 4030\tAvg Score: 0.0646, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.1043/0.1244, pk: 2432, mxpk: 53\n",
      "Episode 4040\tAvg Score: 0.0648, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1083/0.1244, pk: 2442, mxpk: 53\n",
      "Episode 4050\tAvg Score: 0.0649, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1090/0.1244, pk: 2452, mxpk: 53\n",
      "Episode 4060\tAvg Score: 0.0651, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1110/0.1244, pk: 2462, mxpk: 53\n",
      "Episode 4070\tAvg Score: 0.0652, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1140/0.1244, pk: 2472, mxpk: 53\n",
      "Episode 4080\tAvg Score: 0.0653, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1145/0.1244, pk: 2482, mxpk: 53\n",
      "Episode 4090\tAvg Score: 0.0654, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1133/0.1244, pk: 2492, mxpk: 53\n",
      "Episode 4100\tAvg Score: 0.0655, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1118/0.1244, pk: 2502, mxpk: 53\n",
      "Episode 4110\tAvg Score: 0.0656, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1119/0.1244, pk: 2512, mxpk: 53\n",
      "Episode 4120\tAvg Score: 0.0658, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1160/0.1244, pk: 2522, mxpk: 53\n",
      "Episode 4130\tAvg Score: 0.0659, dqmin: 0.0900, dqmax: 0.3000, DQ: 0.1184/0.1244, pk: 2532, mxpk: 53\n",
      "Episode 4140\tAvg Score: 0.0660, dqmin: 0.0900, dqmax: 0.3000, DQ: 0.1143/0.1244, pk: 2542, mxpk: 53\n",
      "Episode 4150\tAvg Score: 0.0661, dqmin: 0.0900, dqmax: 0.3000, DQ: 0.1121/0.1244, pk: 2552, mxpk: 53\n",
      "Episode 4160\tAvg Score: 0.0662, dqmin: 0.0900, dqmax: 0.3000, DQ: 0.1130/0.1244, pk: 2562, mxpk: 53\n",
      "Episode 4170\tAvg Score: 0.0664, dqmin: 0.0900, dqmax: 0.3000, DQ: 0.1119/0.1244, pk: 2572, mxpk: 53\n",
      "Episode 4180\tAvg Score: 0.0664, dqmin: 0.0900, dqmax: 0.3000, DQ: 0.1102/0.1244, pk: 2582, mxpk: 53\n",
      "Episode 4190\tAvg Score: 0.0665, dqmin: 0.0900, dqmax: 0.3000, DQ: 0.1125/0.1244, pk: 2592, mxpk: 53\n",
      "Episode 4200\tAvg Score: 0.0666, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1112/0.1244, pk: 2601, mxpk: 53\n",
      "Episode 4210\tAvg Score: 0.0667, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1093/0.1244, pk: 2610, mxpk: 53\n",
      "Episode 4220\tAvg Score: 0.0668, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1065/0.1244, pk: 2620, mxpk: 53\n",
      "Episode 4230\tAvg Score: 0.0669, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1067/0.1244, pk: 2630, mxpk: 53\n",
      "Episode 4240\tAvg Score: 0.0670, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1093/0.1244, pk: 2640, mxpk: 53\n",
      "Episode 4250\tAvg Score: 0.0671, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1104/0.1244, pk: 2650, mxpk: 53\n",
      "Episode 4260\tAvg Score: 0.0672, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1084/0.1244, pk: 2660, mxpk: 53\n",
      "Episode 4270\tAvg Score: 0.0674, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1097/0.1244, pk: 2670, mxpk: 53\n",
      "Episode 4280\tAvg Score: 0.0675, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1109/0.1244, pk: 2680, mxpk: 53\n",
      "Episode 4290\tAvg Score: 0.0676, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1138/0.1244, pk: 2690, mxpk: 53\n",
      "Episode 4300\tAvg Score: 0.0678, dqmin: 0.0000, dqmax: 0.5000, DQ: 0.1206/0.1244, pk: 2700, mxpk: 54\n",
      "Episode 4310\tAvg Score: 0.0680, dqmin: 0.0900, dqmax: 0.5000, DQ: 0.1234/0.1244, pk: 2710, mxpk: 54\n",
      "Episode 4320\tAvg Score: 0.0681, dqmin: 0.0900, dqmax: 0.5000, DQ: 0.1253/0.1244, pk: 2720, mxpk: 54\n",
      "Mean score: 0.1253\t, old best: 0.1244, max_episode: 204\n",
      "\n",
      "Episode 4330\tAvg Score: 0.0682, dqmin: 0.0900, dqmax: 0.5000, DQ: 0.1263/0.1253, pk: 2730, mxpk: 54\n",
      "Mean score: 0.1263\t, old best: 0.1253, max_episode: 204\n",
      "\n",
      "Episode 4340\tAvg Score: 0.0685, dqmin: 0.0900, dqmax: 0.5000, DQ: 0.1292/0.1263, pk: 2740, mxpk: 54\n",
      "Mean score: 0.1292\t, old best: 0.1263, max_episode: 204\n",
      "\n",
      "Episode 4350\tAvg Score: 0.0686, dqmin: 0.0900, dqmax: 0.5000, DQ: 0.1323/0.1292, pk: 2750, mxpk: 54\n",
      "Mean score: 0.1323\t, old best: 0.1292, max_episode: 204\n",
      "\n",
      "Episode 4360\tAvg Score: 0.0687, dqmin: 0.0900, dqmax: 0.5000, DQ: 0.1335/0.1323, pk: 2760, mxpk: 54\n",
      "Mean score: 0.1335\t, old best: 0.1323, max_episode: 204\n",
      "\n",
      "Episode 4370\tAvg Score: 0.0688, dqmin: 0.0900, dqmax: 0.5000, DQ: 0.1322/0.1335, pk: 2770, mxpk: 54\n",
      "Episode 4380\tAvg Score: 0.0691, dqmin: 0.0900, dqmax: 0.5000, DQ: 0.1404/0.1335, pk: 2780, mxpk: 54\n",
      "Mean score: 0.1404\t, old best: 0.1335, max_episode: 204\n",
      "\n",
      "Episode 4390\tAvg Score: 0.0701, dqmin: 0.0900, dqmax: 2.6000, DQ: 0.1764/0.1404, pk: 2790, mxpk: 56\n",
      "Mean score: 0.1764\t, old best: 0.1404, max_episode: 1000\n",
      "\n",
      "Episode 4400\tAvg Score: 0.0702, dqmin: 0.0900, dqmax: 2.6000, DQ: 0.1714/0.1764, pk: 2800, mxpk: 56\n",
      "Episode 4410\tAvg Score: 0.0709, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.1979/0.1764, pk: 2809, mxpk: 56\n",
      "Mean score: 0.1979\t, old best: 0.1764, max_episode: 1000\n",
      "\n",
      "Episode 4420\tAvg Score: 0.0716, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2202/0.1979, pk: 2819, mxpk: 56\n",
      "Mean score: 0.2202\t, old best: 0.1979, max_episode: 1000\n",
      "\n",
      "Episode 4430\tAvg Score: 0.0724, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2534/0.2202, pk: 2829, mxpk: 56\n",
      "Mean score: 0.2534\t, old best: 0.2202, max_episode: 1000\n",
      "\n",
      "Episode 4440\tAvg Score: 0.0730, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2686/0.2534, pk: 2839, mxpk: 56\n",
      "Mean score: 0.2686\t, old best: 0.2534, max_episode: 1000\n",
      "\n",
      "Episode 4450\tAvg Score: 0.0744, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.3279/0.2686, pk: 2849, mxpk: 56\n",
      "Mean score: 0.3279\t, old best: 0.2686, max_episode: 1000\n",
      "\n",
      "Episode 4460\tAvg Score: 0.0750, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.3470/0.3279, pk: 2859, mxpk: 56\n",
      "Mean score: 0.3470\t, old best: 0.3279, max_episode: 1000\n",
      "\n",
      "Episode 4470\tAvg Score: 0.0755, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.3651/0.3470, pk: 2869, mxpk: 56\n",
      "Mean score: 0.3651\t, old best: 0.3470, max_episode: 1000\n",
      "\n",
      "Episode 4480\tAvg Score: 0.0769, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.4192/0.3651, pk: 2879, mxpk: 58\n",
      "Mean score: 0.4192\t, old best: 0.3651, max_episode: 1000\n",
      "\n",
      "Episode 4490\tAvg Score: 0.0772, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.3903/0.4192, pk: 2889, mxpk: 58\n",
      "Episode 4500\tAvg Score: 0.0774, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.3953/0.4192, pk: 2899, mxpk: 58\n",
      "Episode 4510\tAvg Score: 0.0792, dqmin: 0.0900, dqmax: 2.7000, DQ: 0.4429/0.4192, pk: 2909, mxpk: 59\n",
      "Mean score: 0.4429\t, old best: 0.4192, max_episode: 1000\n",
      "\n",
      "Requirement met on Episode 4519, Requirement Average Score: 0.0811, mxeps1000\n",
      "\n",
      "Episode 4520\tAvg Score: 0.0811, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.5019/0.4429, pk: 2918, mxpk: 60\n",
      "Requirement met on Episode 4520, Requirement Average Score: 0.0811, mxeps1000\n",
      "\n",
      "Mean score: 0.5019\t, old best: 0.4429, max_episode: 1000\n",
      "\n",
      "Episode 4530\tAvg Score: 0.0812, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.4687/0.5019, pk: 2928, mxpk: 60\n",
      "Episode 4540\tAvg Score: 0.0816, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.4634/0.5019, pk: 2938, mxpk: 60\n",
      "Episode 4550\tAvg Score: 0.0816, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.4021/0.5019, pk: 2948, mxpk: 60\n",
      "Episode 4560\tAvg Score: 0.0820, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.3969/0.5019, pk: 2958, mxpk: 60\n",
      "Episode 4570\tAvg Score: 0.0822, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.3820/0.5019, pk: 2968, mxpk: 60\n",
      "Episode 4580\tAvg Score: 0.0824, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.3257/0.5019, pk: 2978, mxpk: 60\n",
      "Episode 4590\tAvg Score: 0.0827, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.3258/0.5019, pk: 2988, mxpk: 60\n",
      "Episode 4600\tAvg Score: 0.0827, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.3208/0.5019, pk: 2998, mxpk: 60\n",
      "Episode 4610\tAvg Score: 0.0833, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.2689/0.5019, pk: 3008, mxpk: 60\n",
      "Episode 4620\tAvg Score: 0.0836, dqmin: 0.0900, dqmax: 2.5000, DQ: 0.1987/0.5019, pk: 3018, mxpk: 60\n",
      "Episode 4630\tAvg Score: 0.0837, dqmin: 0.0900, dqmax: 2.5000, DQ: 0.1998/0.5019, pk: 3028, mxpk: 60\n",
      "Episode 4640\tAvg Score: 0.0838, dqmin: 0.0900, dqmax: 2.5000, DQ: 0.1878/0.5019, pk: 3038, mxpk: 60\n",
      "Episode 4650\tAvg Score: 0.0840, dqmin: 0.0900, dqmax: 2.5000, DQ: 0.1901/0.5019, pk: 3048, mxpk: 60\n",
      "Episode 4660\tAvg Score: 0.0840, dqmin: 0.0900, dqmax: 2.5000, DQ: 0.1748/0.5019, pk: 3058, mxpk: 60\n",
      "Episode 4670\tAvg Score: 0.0844, dqmin: 0.0900, dqmax: 2.5000, DQ: 0.1868/0.5019, pk: 3068, mxpk: 60\n",
      "Episode 4680\tAvg Score: 0.0844, dqmin: 0.0900, dqmax: 2.5000, DQ: 0.1799/0.5019, pk: 3078, mxpk: 60\n",
      "Episode 4690\tAvg Score: 0.0845, dqmin: 0.0900, dqmax: 2.5000, DQ: 0.1708/0.5019, pk: 3088, mxpk: 60\n",
      "Episode 4700\tAvg Score: 0.0846, dqmin: 0.0900, dqmax: 2.5000, DQ: 0.1698/0.5019, pk: 3098, mxpk: 60\n",
      "Episode 4710\tAvg Score: 0.0846, dqmin: 0.0900, dqmax: 1.2000, DQ: 0.1446/0.5019, pk: 3108, mxpk: 60\n",
      "Episode 4720\tAvg Score: 0.0846, dqmin: 0.0900, dqmax: 1.2000, DQ: 0.1318/0.5019, pk: 3118, mxpk: 60\n",
      "Episode 4730\tAvg Score: 0.0847, dqmin: 0.0900, dqmax: 1.2000, DQ: 0.1287/0.5019, pk: 3128, mxpk: 60\n",
      "Episode 4740\tAvg Score: 0.0848, dqmin: 0.0900, dqmax: 1.2000, DQ: 0.1297/0.5019, pk: 3138, mxpk: 60\n",
      "Episode 4750\tAvg Score: 0.0849, dqmin: 0.0900, dqmax: 1.2000, DQ: 0.1264/0.5019, pk: 3148, mxpk: 60\n",
      "Episode 4760\tAvg Score: 0.0849, dqmin: 0.0900, dqmax: 1.2000, DQ: 0.1276/0.5019, pk: 3158, mxpk: 60\n",
      "Episode 4770\tAvg Score: 0.0850, dqmin: 0.0900, dqmax: 0.4000, DQ: 0.1113/0.5019, pk: 3168, mxpk: 60\n",
      "Episode 4780\tAvg Score: 0.0850, dqmin: 0.0900, dqmax: 0.4000, DQ: 0.1109/0.5019, pk: 3178, mxpk: 60\n",
      "Episode 4790\tAvg Score: 0.0851, dqmin: 0.0900, dqmax: 0.2000, DQ: 0.1118/0.5019, pk: 3188, mxpk: 60\n",
      "Episode 4800\tAvg Score: 0.0852, dqmin: 0.0900, dqmax: 0.2000, DQ: 0.1137/0.5019, pk: 3198, mxpk: 60\n",
      "Episode 4810\tAvg Score: 0.0853, dqmin: 0.0900, dqmax: 0.3000, DQ: 0.1199/0.5019, pk: 3208, mxpk: 60\n",
      "Episode 4820\tAvg Score: 0.0856, dqmin: 0.0000, dqmax: 0.8000, DQ: 0.1307/0.5019, pk: 3217, mxpk: 60\n",
      "Episode 4830\tAvg Score: 0.0857, dqmin: 0.0000, dqmax: 0.8000, DQ: 0.1337/0.5019, pk: 3227, mxpk: 60\n",
      "Episode 4840\tAvg Score: 0.0858, dqmin: 0.0000, dqmax: 0.8000, DQ: 0.1328/0.5019, pk: 3237, mxpk: 60\n",
      "Episode 4850\tAvg Score: 0.0862, dqmin: 0.0000, dqmax: 0.8000, DQ: 0.1489/0.5019, pk: 3247, mxpk: 60\n",
      "Episode 4860\tAvg Score: 0.0873, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.1991/0.5019, pk: 3257, mxpk: 60\n",
      "Episode 4870\tAvg Score: 0.0876, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2136/0.5019, pk: 3267, mxpk: 60\n",
      "Episode 4880\tAvg Score: 0.0882, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2390/0.5019, pk: 3277, mxpk: 60\n",
      "Episode 4890\tAvg Score: 0.0882, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2380/0.5019, pk: 3287, mxpk: 60\n",
      "Episode 4900\tAvg Score: 0.0883, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2362/0.5019, pk: 3297, mxpk: 60\n",
      "Episode 4910\tAvg Score: 0.0883, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2313/0.5019, pk: 3307, mxpk: 60\n",
      "Episode 4920\tAvg Score: 0.0883, dqmin: 0.0900, dqmax: 2.6000, DQ: 0.2192/0.5019, pk: 3317, mxpk: 60\n",
      "Episode 4930\tAvg Score: 0.0883, dqmin: 0.0900, dqmax: 2.6000, DQ: 0.2154/0.5019, pk: 3327, mxpk: 60\n",
      "Episode 4940\tAvg Score: 0.0883, dqmin: 0.0900, dqmax: 2.6000, DQ: 0.2113/0.5019, pk: 3337, mxpk: 60\n",
      "Episode 4950\tAvg Score: 0.0883, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.1935/0.5019, pk: 3346, mxpk: 60\n",
      "Episode 4960\tAvg Score: 0.0889, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.1675/0.5019, pk: 3356, mxpk: 60\n",
      "Episode 4970\tAvg Score: 0.0890, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.1543/0.5019, pk: 3366, mxpk: 60\n",
      "Episode 4980\tAvg Score: 0.0891, dqmin: 0.0000, dqmax: 1.0000, DQ: 0.1355/0.5019, pk: 3376, mxpk: 60\n",
      "Episode 4990\tAvg Score: 0.0892, dqmin: 0.0000, dqmax: 1.0000, DQ: 0.1367/0.5019, pk: 3386, mxpk: 60\n",
      "Episode 5000\tAvg Score: 0.0893, dqmin: 0.0000, dqmax: 1.0000, DQ: 0.1425/0.5019, pk: 3396, mxpk: 60\n",
      "Episode 5010\tAvg Score: 0.0894, dqmin: 0.0000, dqmax: 1.0000, DQ: 0.1435/0.5019, pk: 3406, mxpk: 60\n",
      "Episode 5020\tAvg Score: 0.0894, dqmin: 0.0000, dqmax: 1.0000, DQ: 0.1438/0.5019, pk: 3415, mxpk: 60\n",
      "Episode 5030\tAvg Score: 0.0894, dqmin: 0.0000, dqmax: 1.0000, DQ: 0.1436/0.5019, pk: 3425, mxpk: 60\n",
      "Episode 5040\tAvg Score: 0.0895, dqmin: 0.0000, dqmax: 1.0000, DQ: 0.1447/0.5019, pk: 3435, mxpk: 60\n",
      "Episode 5050\tAvg Score: 0.0895, dqmin: 0.0000, dqmax: 1.0000, DQ: 0.1466/0.5019, pk: 3445, mxpk: 60\n",
      "Episode 5060\tAvg Score: 0.0896, dqmin: 0.0000, dqmax: 0.3000, DQ: 0.1236/0.5019, pk: 3455, mxpk: 60\n",
      "Episode 5070\tAvg Score: 0.0898, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1298/0.5019, pk: 3465, mxpk: 60\n",
      "Episode 5080\tAvg Score: 0.0898, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1236/0.5019, pk: 3475, mxpk: 60\n",
      "Episode 5090\tAvg Score: 0.0899, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1226/0.5019, pk: 3485, mxpk: 60\n",
      "Episode 5100\tAvg Score: 0.0899, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1177/0.5019, pk: 3495, mxpk: 60\n",
      "Episode 5110\tAvg Score: 0.0900, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1192/0.5019, pk: 3505, mxpk: 60\n",
      "Episode 5120\tAvg Score: 0.0900, dqmin: 0.0900, dqmax: 0.6000, DQ: 0.1211/0.5019, pk: 3515, mxpk: 60\n",
      "Episode 5130\tAvg Score: 0.0900, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1210/0.5019, pk: 3524, mxpk: 60\n",
      "Episode 5140\tAvg Score: 0.0901, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1198/0.5019, pk: 3534, mxpk: 60\n",
      "Episode 5150\tAvg Score: 0.0902, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1235/0.5019, pk: 3544, mxpk: 60\n",
      "Episode 5160\tAvg Score: 0.0903, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1242/0.5019, pk: 3554, mxpk: 60\n",
      "Episode 5170\tAvg Score: 0.0903, dqmin: 0.0000, dqmax: 0.4900, DQ: 0.1194/0.5019, pk: 3564, mxpk: 60\n",
      "Episode 5180\tAvg Score: 0.0906, dqmin: 0.0000, dqmax: 0.4900, DQ: 0.1313/0.5019, pk: 3574, mxpk: 60\n",
      "Episode 5190\tAvg Score: 0.0907, dqmin: 0.0000, dqmax: 0.4900, DQ: 0.1311/0.5019, pk: 3584, mxpk: 60\n",
      "Episode 5200\tAvg Score: 0.0907, dqmin: 0.0000, dqmax: 0.4900, DQ: 0.1339/0.5019, pk: 3593, mxpk: 60\n",
      "Episode 5210\tAvg Score: 0.0909, dqmin: 0.0000, dqmax: 0.4000, DQ: 0.1394/0.5019, pk: 3603, mxpk: 60\n",
      "Episode 5220\tAvg Score: 0.0910, dqmin: 0.0000, dqmax: 0.4000, DQ: 0.1404/0.5019, pk: 3613, mxpk: 60\n",
      "Episode 5230\tAvg Score: 0.0911, dqmin: 0.0000, dqmax: 0.4000, DQ: 0.1456/0.5019, pk: 3623, mxpk: 60\n",
      "Episode 5240\tAvg Score: 0.0915, dqmin: 0.0000, dqmax: 0.8000, DQ: 0.1639/0.5019, pk: 3633, mxpk: 60\n",
      "Episode 5250\tAvg Score: 0.0917, dqmin: 0.0000, dqmax: 0.9000, DQ: 0.1722/0.5019, pk: 3642, mxpk: 60\n",
      "Episode 5260\tAvg Score: 0.0919, dqmin: 0.0000, dqmax: 0.9000, DQ: 0.1755/0.5019, pk: 3652, mxpk: 60\n",
      "Episode 5270\tAvg Score: 0.0921, dqmin: 0.0000, dqmax: 0.9000, DQ: 0.1830/0.5019, pk: 3662, mxpk: 60\n",
      "Episode 5280\tAvg Score: 0.0924, dqmin: 0.0000, dqmax: 0.9000, DQ: 0.1863/0.5019, pk: 3672, mxpk: 60\n",
      "Episode 5290\tAvg Score: 0.0925, dqmin: 0.0000, dqmax: 0.9000, DQ: 0.1867/0.5019, pk: 3682, mxpk: 60\n",
      "Episode 5300\tAvg Score: 0.0926, dqmin: 0.0000, dqmax: 0.9000, DQ: 0.1902/0.5019, pk: 3692, mxpk: 60\n",
      "Episode 5310\tAvg Score: 0.0927, dqmin: 0.0000, dqmax: 0.9000, DQ: 0.1851/0.5019, pk: 3702, mxpk: 60\n",
      "Episode 5320\tAvg Score: 0.0929, dqmin: 0.0000, dqmax: 0.9000, DQ: 0.1944/0.5019, pk: 3711, mxpk: 60\n",
      "Episode 5330\tAvg Score: 0.0931, dqmin: 0.0000, dqmax: 0.9000, DQ: 0.1956/0.5019, pk: 3720, mxpk: 60\n",
      "Episode 5340\tAvg Score: 0.0934, dqmin: 0.0000, dqmax: 1.1000, DQ: 0.1925/0.5019, pk: 3730, mxpk: 60\n",
      "Episode 5350\tAvg Score: 0.0935, dqmin: 0.0000, dqmax: 1.1000, DQ: 0.1855/0.5019, pk: 3740, mxpk: 60\n",
      "Episode 5360\tAvg Score: 0.0936, dqmin: 0.0000, dqmax: 1.1000, DQ: 0.1843/0.5019, pk: 3750, mxpk: 60\n",
      "Episode 5370\tAvg Score: 0.0938, dqmin: 0.0000, dqmax: 1.1000, DQ: 0.1837/0.5019, pk: 3760, mxpk: 60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5380\tAvg Score: 0.0939, dqmin: 0.0000, dqmax: 1.1000, DQ: 0.1717/0.5019, pk: 3770, mxpk: 60\n",
      "Episode 5390\tAvg Score: 0.0941, dqmin: 0.0000, dqmax: 1.1000, DQ: 0.1795/0.5019, pk: 3780, mxpk: 60\n",
      "Episode 5400\tAvg Score: 0.0942, dqmin: 0.0000, dqmax: 1.1000, DQ: 0.1792/0.5019, pk: 3789, mxpk: 60\n",
      "Episode 5410\tAvg Score: 0.0943, dqmin: 0.0000, dqmax: 1.1000, DQ: 0.1814/0.5019, pk: 3799, mxpk: 60\n",
      "Episode 5420\tAvg Score: 0.0944, dqmin: 0.0000, dqmax: 1.1000, DQ: 0.1729/0.5019, pk: 3809, mxpk: 60\n",
      "Episode 5430\tAvg Score: 0.0947, dqmin: 0.0000, dqmax: 1.1000, DQ: 0.1807/0.5019, pk: 3819, mxpk: 60\n",
      "Episode 5440\tAvg Score: 0.0948, dqmin: 0.0000, dqmax: 0.7000, DQ: 0.1745/0.5019, pk: 3829, mxpk: 60\n",
      "Episode 5450\tAvg Score: 0.0951, dqmin: 0.0000, dqmax: 0.7000, DQ: 0.1826/0.5019, pk: 3839, mxpk: 60\n",
      "Episode 5460\tAvg Score: 0.0953, dqmin: 0.0000, dqmax: 0.7000, DQ: 0.1858/0.5019, pk: 3848, mxpk: 60\n",
      "Episode 5470\tAvg Score: 0.0956, dqmin: 0.0000, dqmax: 0.8000, DQ: 0.1918/0.5019, pk: 3858, mxpk: 60\n",
      "Episode 5480\tAvg Score: 0.0961, dqmin: 0.0000, dqmax: 0.8000, DQ: 0.2158/0.5019, pk: 3868, mxpk: 60\n",
      "Episode 5490\tAvg Score: 0.0964, dqmin: 0.0000, dqmax: 0.8000, DQ: 0.2207/0.5019, pk: 3878, mxpk: 60\n",
      "Episode 5500\tAvg Score: 0.0967, dqmin: 0.0000, dqmax: 1.3000, DQ: 0.2308/0.5019, pk: 3888, mxpk: 60\n",
      "Episode 5510\tAvg Score: 0.0969, dqmin: 0.0000, dqmax: 1.3000, DQ: 0.2328/0.5019, pk: 3898, mxpk: 60\n",
      "Episode 5520\tAvg Score: 0.0969, dqmin: 0.0000, dqmax: 1.3000, DQ: 0.2311/0.5019, pk: 3908, mxpk: 60\n",
      "Episode 5530\tAvg Score: 0.0973, dqmin: 0.0000, dqmax: 1.3000, DQ: 0.2394/0.5019, pk: 3918, mxpk: 60\n",
      "Episode 5540\tAvg Score: 0.0975, dqmin: 0.0000, dqmax: 1.3000, DQ: 0.2438/0.5019, pk: 3928, mxpk: 60\n",
      "Episode 5550\tAvg Score: 0.0978, dqmin: 0.0000, dqmax: 1.3000, DQ: 0.2459/0.5019, pk: 3938, mxpk: 60\n",
      "Episode 5560\tAvg Score: 0.0980, dqmin: 0.0900, dqmax: 1.3000, DQ: 0.2449/0.5019, pk: 3948, mxpk: 60\n",
      "Episode 5570\tAvg Score: 0.0981, dqmin: 0.0900, dqmax: 1.3000, DQ: 0.2360/0.5019, pk: 3958, mxpk: 60\n",
      "Episode 5580\tAvg Score: 0.0987, dqmin: 0.0900, dqmax: 1.5000, DQ: 0.2399/0.5019, pk: 3968, mxpk: 60\n",
      "Episode 5590\tAvg Score: 0.0990, dqmin: 0.0900, dqmax: 1.5000, DQ: 0.2402/0.5019, pk: 3978, mxpk: 60\n",
      "Episode 5600\tAvg Score: 0.0990, dqmin: 0.0900, dqmax: 1.5000, DQ: 0.2268/0.5019, pk: 3988, mxpk: 60\n",
      "Episode 5610\tAvg Score: 0.0994, dqmin: 0.0900, dqmax: 1.5000, DQ: 0.2398/0.5019, pk: 3998, mxpk: 60\n",
      "Episode 5620\tAvg Score: 0.0996, dqmin: 0.0900, dqmax: 1.5000, DQ: 0.2496/0.5019, pk: 4008, mxpk: 60\n",
      "Episode 5630\tAvg Score: 0.0999, dqmin: 0.0900, dqmax: 1.5000, DQ: 0.2454/0.5019, pk: 4018, mxpk: 60\n",
      "Episode 5640\tAvg Score: 0.1006, dqmin: 0.0900, dqmax: 1.5000, DQ: 0.2703/0.5019, pk: 4028, mxpk: 60\n",
      "Episode 5650\tAvg Score: 0.1009, dqmin: 0.0900, dqmax: 1.5000, DQ: 0.2689/0.5019, pk: 4038, mxpk: 60\n",
      "Episode 5660\tAvg Score: 0.1012, dqmin: 0.0900, dqmax: 1.5000, DQ: 0.2779/0.5019, pk: 4048, mxpk: 60\n",
      "Episode 5670\tAvg Score: 0.1018, dqmin: 0.0900, dqmax: 2.0000, DQ: 0.3089/0.5019, pk: 4058, mxpk: 60\n",
      "Episode 5680\tAvg Score: 0.1024, dqmin: 0.0900, dqmax: 2.0000, DQ: 0.3107/0.5019, pk: 4068, mxpk: 60\n",
      "Episode 5690\tAvg Score: 0.1027, dqmin: 0.0900, dqmax: 2.0000, DQ: 0.3105/0.5019, pk: 4078, mxpk: 60\n",
      "Episode 5700\tAvg Score: 0.1030, dqmin: 0.0900, dqmax: 2.0000, DQ: 0.3279/0.5019, pk: 4088, mxpk: 60\n",
      "Episode 5710\tAvg Score: 0.1039, dqmin: 0.0900, dqmax: 2.0000, DQ: 0.3549/0.5019, pk: 4098, mxpk: 60\n",
      "Episode 5720\tAvg Score: 0.1038, dqmin: 0.0000, dqmax: 2.0000, DQ: 0.3393/0.5019, pk: 4102, mxpk: 60\n",
      "Episode 5730\tAvg Score: 0.1046, dqmin: 0.0000, dqmax: 2.2000, DQ: 0.3674/0.5019, pk: 4110, mxpk: 60\n",
      "Episode 5740\tAvg Score: 0.1051, dqmin: 0.0000, dqmax: 2.2000, DQ: 0.3614/0.5019, pk: 4120, mxpk: 60\n",
      "Episode 5750\tAvg Score: 0.1056, dqmin: 0.0000, dqmax: 2.2000, DQ: 0.3748/0.5019, pk: 4130, mxpk: 60\n",
      "Episode 5760\tAvg Score: 0.1069, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.4330/0.5019, pk: 4140, mxpk: 60\n",
      "Episode 5770\tAvg Score: 0.1077, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.4429/0.5019, pk: 4150, mxpk: 60\n",
      "Episode 5780\tAvg Score: 0.1078, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.4152/0.5019, pk: 4160, mxpk: 60\n",
      "Episode 5790\tAvg Score: 0.1080, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.4093/0.5019, pk: 4170, mxpk: 60\n",
      "Episode 5800\tAvg Score: 0.1080, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.3930/0.5019, pk: 4180, mxpk: 60\n",
      "Episode 5810\tAvg Score: 0.1081, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.3469/0.5019, pk: 4190, mxpk: 60\n",
      "Episode 5820\tAvg Score: 0.1082, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.3598/0.5019, pk: 4200, mxpk: 60\n",
      "Episode 5830\tAvg Score: 0.1082, dqmin: 0.0900, dqmax: 2.6000, DQ: 0.3178/0.5019, pk: 4210, mxpk: 60\n",
      "Episode 5840\tAvg Score: 0.1084, dqmin: 0.0900, dqmax: 2.6000, DQ: 0.2948/0.5019, pk: 4220, mxpk: 60\n",
      "Episode 5850\tAvg Score: 0.1086, dqmin: 0.0900, dqmax: 2.6000, DQ: 0.2786/0.5019, pk: 4230, mxpk: 60\n",
      "Episode 5860\tAvg Score: 0.1091, dqmin: 0.0900, dqmax: 1.5000, DQ: 0.2353/0.5019, pk: 4240, mxpk: 60\n",
      "Episode 5870\tAvg Score: 0.1092, dqmin: 0.0900, dqmax: 1.5000, DQ: 0.1903/0.5019, pk: 4250, mxpk: 60\n",
      "Episode 5880\tAvg Score: 0.1094, dqmin: 0.0900, dqmax: 1.5000, DQ: 0.2001/0.5019, pk: 4260, mxpk: 60\n",
      "Episode 5890\tAvg Score: 0.1098, dqmin: 0.0900, dqmax: 2.3000, DQ: 0.2148/0.5019, pk: 4270, mxpk: 60\n",
      "Episode 5900\tAvg Score: 0.1098, dqmin: 0.0900, dqmax: 2.3000, DQ: 0.2122/0.5019, pk: 4280, mxpk: 60\n",
      "Episode 5910\tAvg Score: 0.1100, dqmin: 0.0900, dqmax: 2.3000, DQ: 0.2243/0.5019, pk: 4290, mxpk: 60\n",
      "Episode 5920\tAvg Score: 0.1111, dqmin: 0.0900, dqmax: 2.7000, DQ: 0.2824/0.5019, pk: 4300, mxpk: 61\n",
      "Episode 5930\tAvg Score: 0.1111, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.2765/0.5019, pk: 4308, mxpk: 61\n",
      "Episode 5940\tAvg Score: 0.1113, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.2786/0.5019, pk: 4318, mxpk: 61\n",
      "Episode 5950\tAvg Score: 0.1120, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.3118/0.5019, pk: 4328, mxpk: 61\n",
      "Episode 5960\tAvg Score: 0.1129, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.3321/0.5019, pk: 4338, mxpk: 61\n",
      "Episode 5970\tAvg Score: 0.1133, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.3579/0.5019, pk: 4348, mxpk: 61\n",
      "Episode 5980\tAvg Score: 0.1136, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.3640/0.5019, pk: 4358, mxpk: 61\n",
      "Episode 5990\tAvg Score: 0.1137, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.3423/0.5019, pk: 4367, mxpk: 61\n",
      "Episode 6000\tAvg Score: 0.1137, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.3450/0.5019, pk: 4377, mxpk: 61\n",
      "Episode 6010\tAvg Score: 0.1137, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.3311/0.5019, pk: 4387, mxpk: 61\n",
      "Episode 6020\tAvg Score: 0.1137, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2640/0.5019, pk: 4397, mxpk: 61\n",
      "Episode 6030\tAvg Score: 0.1137, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2679/0.5019, pk: 4407, mxpk: 61\n",
      "Episode 6040\tAvg Score: 0.1140, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2778/0.5019, pk: 4417, mxpk: 61\n",
      "Episode 6050\tAvg Score: 0.1141, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2376/0.5019, pk: 4427, mxpk: 61\n",
      "Episode 6060\tAvg Score: 0.1147, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2235/0.5019, pk: 4437, mxpk: 61\n",
      "Episode 6070\tAvg Score: 0.1151, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2217/0.5019, pk: 4447, mxpk: 61\n",
      "Episode 6080\tAvg Score: 0.1151, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2018/0.5019, pk: 4457, mxpk: 61\n",
      "Episode 6090\tAvg Score: 0.1151, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.1986/0.5019, pk: 4466, mxpk: 61\n",
      "Episode 6100\tAvg Score: 0.1151, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.1968/0.5019, pk: 4476, mxpk: 61\n",
      "Episode 6110\tAvg Score: 0.1152, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2037/0.5019, pk: 4486, mxpk: 61\n",
      "Episode 6120\tAvg Score: 0.1152, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2046/0.5019, pk: 4496, mxpk: 61\n",
      "Episode 6130\tAvg Score: 0.1152, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2047/0.5019, pk: 4506, mxpk: 61\n",
      "Episode 6140\tAvg Score: 0.1152, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.1857/0.5019, pk: 4516, mxpk: 61\n",
      "Episode 6150\tAvg Score: 0.1152, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.1827/0.5019, pk: 4526, mxpk: 61\n",
      "Episode 6160\tAvg Score: 0.1152, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.1447/0.5019, pk: 4536, mxpk: 61\n",
      "Episode 6170\tAvg Score: 0.1156, dqmin: 0.0000, dqmax: 1.6000, DQ: 0.1478/0.5019, pk: 4546, mxpk: 61\n",
      "Episode 6180\tAvg Score: 0.1157, dqmin: 0.0000, dqmax: 1.6000, DQ: 0.1497/0.5019, pk: 4556, mxpk: 61\n",
      "Episode 6190\tAvg Score: 0.1159, dqmin: 0.0900, dqmax: 1.6000, DQ: 0.1660/0.5019, pk: 4566, mxpk: 61\n",
      "Episode 6200\tAvg Score: 0.1159, dqmin: 0.0900, dqmax: 1.6000, DQ: 0.1668/0.5019, pk: 4576, mxpk: 61\n",
      "Episode 6210\tAvg Score: 0.1160, dqmin: 0.0900, dqmax: 1.6000, DQ: 0.1637/0.5019, pk: 4586, mxpk: 61\n",
      "Episode 6220\tAvg Score: 0.1163, dqmin: 0.0900, dqmax: 2.2000, DQ: 0.1868/0.5019, pk: 4596, mxpk: 61\n",
      "Episode 6230\tAvg Score: 0.1164, dqmin: 0.0900, dqmax: 2.2000, DQ: 0.1906/0.5019, pk: 4606, mxpk: 61\n",
      "Episode 6240\tAvg Score: 0.1164, dqmin: 0.0900, dqmax: 2.2000, DQ: 0.1897/0.5019, pk: 4616, mxpk: 61\n",
      "Episode 6250\tAvg Score: 0.1164, dqmin: 0.0900, dqmax: 2.2000, DQ: 0.1907/0.5019, pk: 4626, mxpk: 61\n",
      "Episode 6260\tAvg Score: 0.1167, dqmin: 0.0900, dqmax: 2.2000, DQ: 0.2106/0.5019, pk: 4636, mxpk: 61\n",
      "Episode 6270\tAvg Score: 0.1167, dqmin: 0.0900, dqmax: 2.2000, DQ: 0.1825/0.5019, pk: 4646, mxpk: 61\n",
      "Episode 6280\tAvg Score: 0.1169, dqmin: 0.0900, dqmax: 2.2000, DQ: 0.1934/0.5019, pk: 4656, mxpk: 61\n",
      "Episode 6290\tAvg Score: 0.1170, dqmin: 0.0000, dqmax: 2.2000, DQ: 0.1870/0.5019, pk: 4665, mxpk: 61\n",
      "Episode 6300\tAvg Score: 0.1173, dqmin: 0.0000, dqmax: 2.2000, DQ: 0.2023/0.5019, pk: 4674, mxpk: 61\n",
      "Episode 6310\tAvg Score: 0.1176, dqmin: 0.0000, dqmax: 2.2000, DQ: 0.2215/0.5019, pk: 4684, mxpk: 61\n",
      "Episode 6320\tAvg Score: 0.1177, dqmin: 0.0000, dqmax: 1.7000, DQ: 0.2033/0.5019, pk: 4694, mxpk: 61\n",
      "Episode 6330\tAvg Score: 0.1181, dqmin: 0.0000, dqmax: 2.5000, DQ: 0.2242/0.5019, pk: 4704, mxpk: 61\n",
      "Episode 6340\tAvg Score: 0.1183, dqmin: 0.0000, dqmax: 2.5000, DQ: 0.2399/0.5019, pk: 4714, mxpk: 61\n",
      "Episode 6350\tAvg Score: 0.1186, dqmin: 0.0000, dqmax: 2.5000, DQ: 0.2580/0.5019, pk: 4724, mxpk: 61\n",
      "Episode 6360\tAvg Score: 0.1186, dqmin: 0.0000, dqmax: 2.5000, DQ: 0.2392/0.5019, pk: 4734, mxpk: 61\n",
      "Episode 6370\tAvg Score: 0.1186, dqmin: 0.0000, dqmax: 2.5000, DQ: 0.2391/0.5019, pk: 4744, mxpk: 61\n",
      "Episode 6380\tAvg Score: 0.1187, dqmin: 0.0000, dqmax: 2.5000, DQ: 0.2303/0.5019, pk: 4754, mxpk: 61\n",
      "Episode 6390\tAvg Score: 0.1187, dqmin: 0.0000, dqmax: 2.5000, DQ: 0.2215/0.5019, pk: 4764, mxpk: 61\n",
      "Episode 6400\tAvg Score: 0.1189, dqmin: 0.0900, dqmax: 2.5000, DQ: 0.2186/0.5019, pk: 4774, mxpk: 61\n",
      "Episode 6410\tAvg Score: 0.1189, dqmin: 0.0900, dqmax: 2.5000, DQ: 0.1973/0.5019, pk: 4784, mxpk: 61\n",
      "Episode 6420\tAvg Score: 0.1191, dqmin: 0.0000, dqmax: 2.5000, DQ: 0.2065/0.5019, pk: 4792, mxpk: 61\n",
      "Episode 6430\tAvg Score: 0.1190, dqmin: 0.0000, dqmax: 1.7000, DQ: 0.1787/0.5019, pk: 4798, mxpk: 61\n",
      "Episode 6440\tAvg Score: 0.1190, dqmin: 0.0000, dqmax: 1.7000, DQ: 0.1598/0.5019, pk: 4806, mxpk: 61\n",
      "Episode 6450\tAvg Score: 0.1190, dqmin: 0.0000, dqmax: 1.7000, DQ: 0.1415/0.5019, pk: 4816, mxpk: 61\n",
      "Episode 6460\tAvg Score: 0.1190, dqmin: 0.0000, dqmax: 1.7000, DQ: 0.1455/0.5019, pk: 4826, mxpk: 61\n",
      "Episode 6470\tAvg Score: 0.1190, dqmin: 0.0000, dqmax: 1.7000, DQ: 0.1454/0.5019, pk: 4836, mxpk: 61\n",
      "Episode 6480\tAvg Score: 0.1197, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.1871/0.5019, pk: 4846, mxpk: 61\n",
      "Episode 6490\tAvg Score: 0.1197, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.1871/0.5019, pk: 4856, mxpk: 61\n",
      "Episode 6500\tAvg Score: 0.1201, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2009/0.5019, pk: 4866, mxpk: 61\n",
      "Episode 6510\tAvg Score: 0.1203, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2102/0.5019, pk: 4875, mxpk: 61\n",
      "Episode 6520\tAvg Score: 0.1204, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2052/0.5019, pk: 4885, mxpk: 61\n",
      "Episode 6530\tAvg Score: 0.1213, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.2662/0.5019, pk: 4895, mxpk: 62\n",
      "Episode 6540\tAvg Score: 0.1225, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.3493/0.5019, pk: 4905, mxpk: 63\n",
      "Episode 6550\tAvg Score: 0.1229, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.3764/0.5019, pk: 4915, mxpk: 63\n",
      "Episode 6560\tAvg Score: 0.1234, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.4054/0.5019, pk: 4925, mxpk: 63\n",
      "Episode 6570\tAvg Score: 0.1234, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.4054/0.5019, pk: 4934, mxpk: 63\n",
      "Episode 6580\tAvg Score: 0.1237, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.3794/0.5019, pk: 4943, mxpk: 63\n",
      "Episode 6590\tAvg Score: 0.1238, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.3865/0.5019, pk: 4953, mxpk: 63\n",
      "Episode 6600\tAvg Score: 0.1237, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.3563/0.5019, pk: 4961, mxpk: 63\n",
      "Episode 6610\tAvg Score: 0.1239, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.3589/0.5019, pk: 4971, mxpk: 63\n",
      "Episode 6620\tAvg Score: 0.1242, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.3705/0.5019, pk: 4980, mxpk: 63\n",
      "Episode 6630\tAvg Score: 0.1247, dqmin: 0.0000, dqmax: 2.7000, DQ: 0.3465/0.5019, pk: 4990, mxpk: 63\n",
      "Episode 6640\tAvg Score: 0.1247, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2693/0.5019, pk: 5000, mxpk: 63\n",
      "Episode 6650\tAvg Score: 0.1247, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2403/0.5019, pk: 5010, mxpk: 63\n",
      "Episode 6660\tAvg Score: 0.1247, dqmin: 0.0000, dqmax: 2.1000, DQ: 0.2069/0.5019, pk: 5020, mxpk: 63\n",
      "Episode 6670\tAvg Score: 0.1248, dqmin: 0.0000, dqmax: 2.1000, DQ: 0.2150/0.5019, pk: 5030, mxpk: 63\n",
      "Episode 6680\tAvg Score: 0.1248, dqmin: 0.0000, dqmax: 2.1000, DQ: 0.1962/0.5019, pk: 5039, mxpk: 63\n",
      "Episode 6690\tAvg Score: 0.1249, dqmin: 0.0000, dqmax: 2.1000, DQ: 0.2012/0.5019, pk: 5048, mxpk: 63\n",
      "Episode 6700\tAvg Score: 0.1262, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2872/0.5019, pk: 5058, mxpk: 63\n",
      "Episode 6710\tAvg Score: 0.1262, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2794/0.5019, pk: 5068, mxpk: 63\n",
      "Episode 6720\tAvg Score: 0.1262, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2615/0.5019, pk: 5078, mxpk: 63\n",
      "Episode 6730\tAvg Score: 0.1263, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2344/0.5019, pk: 5088, mxpk: 63\n",
      "Episode 6740\tAvg Score: 0.1263, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2332/0.5019, pk: 5098, mxpk: 63\n",
      "Episode 6750\tAvg Score: 0.1265, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2452/0.5019, pk: 5108, mxpk: 63\n",
      "Episode 6760\tAvg Score: 0.1265, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2470/0.5019, pk: 5118, mxpk: 63\n",
      "Episode 6770\tAvg Score: 0.1269, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2690/0.5019, pk: 5128, mxpk: 63\n",
      "Episode 6780\tAvg Score: 0.1269, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2688/0.5019, pk: 5138, mxpk: 63\n",
      "Episode 6790\tAvg Score: 0.1269, dqmin: 0.0900, dqmax: 2.6000, DQ: 0.2575/0.5019, pk: 5148, mxpk: 63\n",
      "Episode 6800\tAvg Score: 0.1269, dqmin: 0.0900, dqmax: 2.6000, DQ: 0.1797/0.5019, pk: 5158, mxpk: 63\n",
      "Episode 6810\tAvg Score: 0.1270, dqmin: 0.0900, dqmax: 2.6000, DQ: 0.1787/0.5019, pk: 5168, mxpk: 63\n",
      "Episode 6820\tAvg Score: 0.1271, dqmin: 0.0900, dqmax: 2.6000, DQ: 0.1878/0.5019, pk: 5178, mxpk: 63\n",
      "Episode 6830\tAvg Score: 0.1271, dqmin: 0.0900, dqmax: 2.6000, DQ: 0.1817/0.5019, pk: 5188, mxpk: 63\n",
      "Episode 6840\tAvg Score: 0.1271, dqmin: 0.0900, dqmax: 2.6000, DQ: 0.1796/0.5019, pk: 5198, mxpk: 63\n",
      "Episode 6850\tAvg Score: 0.1271, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.1686/0.5019, pk: 5206, mxpk: 63\n",
      "Episode 6860\tAvg Score: 0.1273, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.1830/0.5019, pk: 5216, mxpk: 63\n",
      "Episode 6870\tAvg Score: 0.1273, dqmin: 0.0000, dqmax: 1.6000, DQ: 0.1548/0.5019, pk: 5226, mxpk: 63\n",
      "Episode 6880\tAvg Score: 0.1274, dqmin: 0.0000, dqmax: 1.6000, DQ: 0.1620/0.5019, pk: 5236, mxpk: 63\n",
      "Episode 6890\tAvg Score: 0.1274, dqmin: 0.0000, dqmax: 1.6000, DQ: 0.1623/0.5019, pk: 5246, mxpk: 63\n",
      "Episode 6900\tAvg Score: 0.1279, dqmin: 0.0000, dqmax: 2.4000, DQ: 0.1961/0.5019, pk: 5256, mxpk: 63\n",
      "Episode 6910\tAvg Score: 0.1279, dqmin: 0.0000, dqmax: 2.4000, DQ: 0.1941/0.5019, pk: 5266, mxpk: 63\n",
      "Episode 6920\tAvg Score: 0.1280, dqmin: 0.0000, dqmax: 2.4000, DQ: 0.1852/0.5019, pk: 5276, mxpk: 63\n",
      "Episode 6930\tAvg Score: 0.1289, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2525/0.5019, pk: 5285, mxpk: 63\n",
      "Episode 6940\tAvg Score: 0.1289, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2548/0.5019, pk: 5294, mxpk: 63\n",
      "Episode 6950\tAvg Score: 0.1290, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2589/0.5019, pk: 5303, mxpk: 63\n",
      "Episode 6960\tAvg Score: 0.1290, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2451/0.5019, pk: 5313, mxpk: 63\n",
      "Episode 6970\tAvg Score: 0.1291, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2543/0.5019, pk: 5323, mxpk: 63\n",
      "Episode 6980\tAvg Score: 0.1296, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2783/0.5019, pk: 5333, mxpk: 63\n",
      "Episode 6990\tAvg Score: 0.1296, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2823/0.5019, pk: 5343, mxpk: 63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7000\tAvg Score: 0.1300, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2714/0.5019, pk: 5353, mxpk: 63\n",
      "Episode 7010\tAvg Score: 0.1303, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2914/0.5019, pk: 5362, mxpk: 63\n",
      "Episode 7020\tAvg Score: 0.1308, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.3284/0.5019, pk: 5371, mxpk: 63\n",
      "Episode 7030\tAvg Score: 0.1313, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2971/0.5019, pk: 5381, mxpk: 63\n",
      "Episode 7040\tAvg Score: 0.1313, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2958/0.5019, pk: 5391, mxpk: 63\n",
      "Episode 7050\tAvg Score: 0.1317, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.3237/0.5019, pk: 5399, mxpk: 63\n",
      "Episode 7060\tAvg Score: 0.1317, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.3224/0.5019, pk: 5409, mxpk: 63\n",
      "Episode 7070\tAvg Score: 0.1319, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.3253/0.5019, pk: 5419, mxpk: 63\n",
      "Episode 7080\tAvg Score: 0.1319, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2952/0.5019, pk: 5429, mxpk: 63\n",
      "Episode 7090\tAvg Score: 0.1318, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2873/0.5019, pk: 5437, mxpk: 63\n",
      "Episode 7100\tAvg Score: 0.1320, dqmin: 0.0000, dqmax: 2.3000, DQ: 0.2704/0.5019, pk: 5447, mxpk: 63\n",
      "Episode 7110\tAvg Score: 0.1321, dqmin: 0.0000, dqmax: 2.3000, DQ: 0.2636/0.5019, pk: 5457, mxpk: 63\n",
      "Episode 7120\tAvg Score: 0.1321, dqmin: 0.0000, dqmax: 2.3000, DQ: 0.2233/0.5019, pk: 5466, mxpk: 63\n",
      "Episode 7130\tAvg Score: 0.1321, dqmin: 0.0000, dqmax: 2.3000, DQ: 0.1904/0.5019, pk: 5475, mxpk: 63\n",
      "Episode 7140\tAvg Score: 0.1321, dqmin: 0.0000, dqmax: 2.3000, DQ: 0.1908/0.5019, pk: 5485, mxpk: 63\n",
      "Episode 7150\tAvg Score: 0.1321, dqmin: 0.0000, dqmax: 1.6000, DQ: 0.1580/0.5019, pk: 5494, mxpk: 63\n",
      "Episode 7160\tAvg Score: 0.1321, dqmin: 0.0000, dqmax: 1.6000, DQ: 0.1580/0.5019, pk: 5504, mxpk: 63\n",
      "Episode 7170\tAvg Score: 0.1320, dqmin: 0.0000, dqmax: 1.6000, DQ: 0.1411/0.5019, pk: 5512, mxpk: 63\n",
      "Episode 7180\tAvg Score: 0.1320, dqmin: 0.0000, dqmax: 1.6000, DQ: 0.1420/0.5019, pk: 5522, mxpk: 63\n",
      "Episode 7190\tAvg Score: 0.1320, dqmin: 0.0000, dqmax: 1.6000, DQ: 0.1446/0.5019, pk: 5531, mxpk: 63\n",
      "Episode 7200\tAvg Score: 0.1320, dqmin: 0.0000, dqmax: 1.6000, DQ: 0.1327/0.5019, pk: 5541, mxpk: 63\n",
      "Episode 7210\tAvg Score: 0.1320, dqmin: 0.0000, dqmax: 0.5000, DQ: 0.1192/0.5019, pk: 5550, mxpk: 63\n",
      "Episode 7220\tAvg Score: 0.1319, dqmin: 0.0000, dqmax: 0.5000, DQ: 0.1204/0.5019, pk: 5560, mxpk: 63\n",
      "Episode 7230\tAvg Score: 0.1319, dqmin: 0.0000, dqmax: 0.2000, DQ: 0.1170/0.5019, pk: 5570, mxpk: 63\n",
      "Episode 7240\tAvg Score: 0.1320, dqmin: 0.0000, dqmax: 0.3900, DQ: 0.1200/0.5019, pk: 5579, mxpk: 63\n",
      "Episode 7250\tAvg Score: 0.1320, dqmin: 0.0000, dqmax: 0.3900, DQ: 0.1218/0.5019, pk: 5589, mxpk: 63\n",
      "Episode 7260\tAvg Score: 0.1320, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1240/0.5019, pk: 5598, mxpk: 63\n",
      "Episode 7270\tAvg Score: 0.1320, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1279/0.5019, pk: 5608, mxpk: 63\n",
      "Episode 7280\tAvg Score: 0.1320, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1270/0.5019, pk: 5617, mxpk: 63\n",
      "Episode 7290\tAvg Score: 0.1320, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1319/0.5019, pk: 5627, mxpk: 63\n",
      "Episode 7300\tAvg Score: 0.1321, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1396/0.5019, pk: 5637, mxpk: 63\n",
      "Episode 7310\tAvg Score: 0.1321, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1379/0.5019, pk: 5647, mxpk: 63\n",
      "Episode 7320\tAvg Score: 0.1320, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1390/0.5019, pk: 5657, mxpk: 63\n",
      "Episode 7330\tAvg Score: 0.1320, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1373/0.5019, pk: 5665, mxpk: 63\n",
      "Episode 7340\tAvg Score: 0.1320, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1331/0.5019, pk: 5674, mxpk: 63\n",
      "Episode 7350\tAvg Score: 0.1319, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1301/0.5019, pk: 5683, mxpk: 63\n",
      "Episode 7360\tAvg Score: 0.1319, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1260/0.5019, pk: 5691, mxpk: 63\n",
      "Episode 7370\tAvg Score: 0.1318, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1201/0.5019, pk: 5697, mxpk: 63\n",
      "Episode 7380\tAvg Score: 0.1319, dqmin: 0.0000, dqmax: 0.8000, DQ: 0.1241/0.5019, pk: 5706, mxpk: 63\n",
      "Episode 7390\tAvg Score: 0.1322, dqmin: 0.0000, dqmax: 1.3000, DQ: 0.1444/0.5019, pk: 5715, mxpk: 63\n",
      "Episode 7400\tAvg Score: 0.1322, dqmin: 0.0000, dqmax: 1.3000, DQ: 0.1396/0.5019, pk: 5724, mxpk: 63\n",
      "Episode 7410\tAvg Score: 0.1322, dqmin: 0.0000, dqmax: 1.3000, DQ: 0.1425/0.5019, pk: 5734, mxpk: 63\n",
      "Episode 7420\tAvg Score: 0.1321, dqmin: 0.0000, dqmax: 1.3000, DQ: 0.1393/0.5019, pk: 5744, mxpk: 63\n",
      "Episode 7430\tAvg Score: 0.1324, dqmin: 0.0000, dqmax: 1.6000, DQ: 0.1603/0.5019, pk: 5754, mxpk: 63\n",
      "Episode 7440\tAvg Score: 0.1324, dqmin: 0.0000, dqmax: 1.6000, DQ: 0.1625/0.5019, pk: 5763, mxpk: 63\n",
      "Episode 7450\tAvg Score: 0.1325, dqmin: 0.0000, dqmax: 1.6000, DQ: 0.1755/0.5019, pk: 5773, mxpk: 63\n",
      "Episode 7460\tAvg Score: 0.1325, dqmin: 0.0000, dqmax: 1.6000, DQ: 0.1796/0.5019, pk: 5782, mxpk: 63\n",
      "Episode 7470\tAvg Score: 0.1326, dqmin: 0.0000, dqmax: 1.6000, DQ: 0.1895/0.5019, pk: 5791, mxpk: 63\n",
      "Episode 7480\tAvg Score: 0.1326, dqmin: 0.0000, dqmax: 1.6000, DQ: 0.1842/0.5019, pk: 5800, mxpk: 63\n",
      "Episode 7490\tAvg Score: 0.1325, dqmin: 0.0000, dqmax: 1.6000, DQ: 0.1591/0.5019, pk: 5810, mxpk: 63\n",
      "Episode 7500\tAvg Score: 0.1325, dqmin: 0.0000, dqmax: 1.6000, DQ: 0.1560/0.5019, pk: 5820, mxpk: 63\n",
      "Episode 7510\tAvg Score: 0.1324, dqmin: 0.0000, dqmax: 1.6000, DQ: 0.1489/0.5019, pk: 5827, mxpk: 63\n",
      "Episode 7520\tAvg Score: 0.1324, dqmin: 0.0000, dqmax: 1.6000, DQ: 0.1491/0.5019, pk: 5836, mxpk: 63\n",
      "Episode 7530\tAvg Score: 0.1324, dqmin: 0.0000, dqmax: 1.2000, DQ: 0.1311/0.5019, pk: 5845, mxpk: 63\n",
      "Episode 7540\tAvg Score: 0.1324, dqmin: 0.0000, dqmax: 1.2000, DQ: 0.1332/0.5019, pk: 5853, mxpk: 63\n",
      "Episode 7550\tAvg Score: 0.1324, dqmin: 0.0000, dqmax: 0.7000, DQ: 0.1235/0.5019, pk: 5862, mxpk: 63\n",
      "Episode 7560\tAvg Score: 0.1324, dqmin: 0.0000, dqmax: 0.7000, DQ: 0.1244/0.5019, pk: 5871, mxpk: 63\n",
      "Episode 7570\tAvg Score: 0.1324, dqmin: 0.0000, dqmax: 0.7000, DQ: 0.1194/0.5019, pk: 5881, mxpk: 63\n",
      "Episode 7580\tAvg Score: 0.1324, dqmin: 0.0000, dqmax: 0.7000, DQ: 0.1174/0.5019, pk: 5890, mxpk: 63\n",
      "Episode 7590\tAvg Score: 0.1323, dqmin: 0.0000, dqmax: 0.7000, DQ: 0.1174/0.5019, pk: 5900, mxpk: 63\n",
      "Episode 7600\tAvg Score: 0.1324, dqmin: 0.0000, dqmax: 0.7000, DQ: 0.1236/0.5019, pk: 5910, mxpk: 63\n",
      "Episode 7610\tAvg Score: 0.1323, dqmin: 0.0000, dqmax: 0.7000, DQ: 0.1270/0.5019, pk: 5920, mxpk: 63\n",
      "Episode 7620\tAvg Score: 0.1323, dqmin: 0.0000, dqmax: 0.7000, DQ: 0.1257/0.5019, pk: 5929, mxpk: 63\n",
      "Episode 7630\tAvg Score: 0.1323, dqmin: 0.0000, dqmax: 0.7000, DQ: 0.1269/0.5019, pk: 5939, mxpk: 63\n",
      "Episode 7640\tAvg Score: 0.1323, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1256/0.5019, pk: 5948, mxpk: 63\n",
      "Episode 7650\tAvg Score: 0.1323, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1232/0.5019, pk: 5958, mxpk: 63\n",
      "Episode 7660\tAvg Score: 0.1323, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1202/0.5019, pk: 5968, mxpk: 63\n",
      "Episode 7670\tAvg Score: 0.1322, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1202/0.5019, pk: 5978, mxpk: 63\n",
      "Episode 7680\tAvg Score: 0.1322, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1235/0.5019, pk: 5988, mxpk: 63\n",
      "Episode 7690\tAvg Score: 0.1322, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1237/0.5019, pk: 5996, mxpk: 63\n",
      "Episode 7700\tAvg Score: 0.1322, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1155/0.5019, pk: 6004, mxpk: 63\n",
      "Episode 7710\tAvg Score: 0.1322, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1193/0.5019, pk: 6014, mxpk: 63\n",
      "Episode 7720\tAvg Score: 0.1322, dqmin: 0.0000, dqmax: 0.6000, DQ: 0.1285/0.5019, pk: 6024, mxpk: 63\n",
      "Episode 7730\tAvg Score: 0.1324, dqmin: 0.0000, dqmax: 0.7000, DQ: 0.1382/0.5019, pk: 6033, mxpk: 63\n",
      "Episode 7740\tAvg Score: 0.1323, dqmin: 0.0000, dqmax: 0.7000, DQ: 0.1352/0.5019, pk: 6042, mxpk: 63\n",
      "Episode 7750\tAvg Score: 0.1323, dqmin: 0.0000, dqmax: 0.7000, DQ: 0.1332/0.5019, pk: 6050, mxpk: 63\n",
      "Episode 7760\tAvg Score: 0.1323, dqmin: 0.0000, dqmax: 0.7000, DQ: 0.1311/0.5019, pk: 6059, mxpk: 63\n",
      "Episode 7770\tAvg Score: 0.1323, dqmin: 0.0000, dqmax: 1.1000, DQ: 0.1402/0.5019, pk: 6068, mxpk: 63\n",
      "Episode 7780\tAvg Score: 0.1324, dqmin: 0.0000, dqmax: 1.1000, DQ: 0.1421/0.5019, pk: 6077, mxpk: 63\n",
      "Episode 7790\tAvg Score: 0.1325, dqmin: 0.0000, dqmax: 1.1000, DQ: 0.1530/0.5019, pk: 6087, mxpk: 63\n",
      "Episode 7800\tAvg Score: 0.1325, dqmin: 0.0000, dqmax: 1.1000, DQ: 0.1619/0.5019, pk: 6097, mxpk: 63\n",
      "Episode 7810\tAvg Score: 0.1328, dqmin: 0.0000, dqmax: 1.3000, DQ: 0.1780/0.5019, pk: 6107, mxpk: 63\n",
      "Episode 7820\tAvg Score: 0.1327, dqmin: 0.0000, dqmax: 1.3000, DQ: 0.1699/0.5019, pk: 6116, mxpk: 63\n",
      "Episode 7830\tAvg Score: 0.1329, dqmin: 0.0000, dqmax: 1.3000, DQ: 0.1702/0.5019, pk: 6126, mxpk: 63\n",
      "Episode 7840\tAvg Score: 0.1330, dqmin: 0.0000, dqmax: 1.3000, DQ: 0.1853/0.5019, pk: 6136, mxpk: 63\n",
      "Episode 7850\tAvg Score: 0.1334, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2212/0.5019, pk: 6146, mxpk: 63\n",
      "Episode 7860\tAvg Score: 0.1340, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.2675/0.5019, pk: 6156, mxpk: 63\n",
      "Episode 7870\tAvg Score: 0.1347, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.3184/0.5019, pk: 6166, mxpk: 63\n",
      "Episode 7880\tAvg Score: 0.1354, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.3726/0.5019, pk: 6176, mxpk: 63\n",
      "Episode 7890\tAvg Score: 0.1361, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.4188/0.5019, pk: 6186, mxpk: 63\n",
      "Episode 7900\tAvg Score: 0.1371, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.4960/0.5019, pk: 6196, mxpk: 63\n",
      "Requirement met on Episode 7909, Requirement Average Score: 0.1376, mxeps1000\n",
      "\n",
      "Episode 7910\tAvg Score: 0.1379, dqmin: 0.0000, dqmax: 2.6000, DQ: 0.5430/0.5019, pk: 6206, mxpk: 63\n",
      "Requirement met on Episode 7910, Requirement Average Score: 0.1379, mxeps1000\n",
      "\n",
      "Mean score: 0.5430\t, old best: 0.5019, max_episode: 1000\n",
      "\n",
      "Requirement met on Episode 7911, Requirement Average Score: 0.1380, mxeps1000\n",
      "\n",
      "Final Episode 7911, Final Average Score: 0.1380, mxeps1000\n",
      "\n",
      "Final Best DGave:0.5430\n"
     ]
    }
   ],
   "source": [
    "# use keep_awake to keep workspace from disconnecting\n",
    "tstart = datetime.datetime.now()\n",
    "nscores_deque = deque(maxlen=moving_avg)\n",
    "nbest_mean = -100\n",
    "## KAE 4/12/22: needed?\n",
    "t = 0\n",
    "max_episode_t = -1\n",
    "nscores = []\n",
    "nmean_scores = []\n",
    "# KAE 4/12/2022: don't think we need for our application either agent0 or 1\n",
    "scores =  []\n",
    "cnt_pos_scores = 0\n",
    "cnt_pos_mxscores = 0\n",
    "last_max = 1.0e-6\n",
    "for i in range(num_agents):\n",
    "    maddpg_agents[i].reset() #noise reset for each agent\n",
    "\n",
    "#for episode in keep_awake(range(0, number_of_episodes, parallel_envs)):\n",
    "for episode in keep_awake(range(0, number_of_episodes)):\n",
    "\n",
    "    # KAE 4/11/2022: actual envrionment from above random setting to initalize the env_info\n",
    "    #  The states from the env_info\n",
    "    #  and the initialized scores\n",
    "    env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "    # KAE 4/11/2022: get the initial state for each agent....\n",
    "    h = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    score = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "\n",
    "    # KAE 4/14/2022: based on \n",
    "    \n",
    "    #for calculating rewards for this particular episode - addition of all time steps\n",
    "    # save info or not\n",
    "    save_info = ((episode) % save_interval < 1 or episode==number_of_episodes-1)\n",
    "    tmax = 0\n",
    "\n",
    "#    if save_info:\n",
    "#        for i in range(num_agents):\n",
    "#            maddpg_agents[i].reset() #noise reset for each agent\n",
    "    \n",
    "    episode_t = 0\n",
    "# temp values for monitoring action values....\n",
    "#    min_action = 1.0e38\n",
    "#    max_action = -1.0e38\n",
    "#    for episode_t in range(episode_length):\n",
    "    while True:\n",
    "        t += 1\n",
    "        # explore = only explore for a certain number of episodes\n",
    "        # action input needs to be transposed\n",
    "# KAE 4/12/22: get all actions (list) for each agent(2) in maddpg\n",
    "        actions = [maddpg_agents[i].act(states[i], add_noise=True) for i in range(num_agents)]\n",
    "#        print('action, t',actions, t, episode)\n",
    "#        min_actionc = np.min(actions)\n",
    "#        max_actionc = np.max(actions)\n",
    "#        min_action = np.min([min_action, min_actionc])\n",
    "#        max_action = np.max([max_action, max_actionc])\n",
    "\n",
    "#        noise *= noise_reduction\n",
    "# KAE 4/12/22: maddpg.act is already clipped, so not needed here....\n",
    "        \n",
    "        # step forward one frame\n",
    "# KAE 4/12/22: previous env was custom made, here we are using the unity env which has a standard output\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "#        next_states = np.array(env_info.vector_observations)\n",
    "#        rewards = np.array(env_info.rewards)\n",
    "#        dones = np.array(env_info.local_done)\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        #KAE 4/14/22: we note that step here passes in the FULL states[0:1] whereas elsewhere independent\n",
    "        for i in range(num_agents):\n",
    "            maddpg_agents[i].step(states[i], actions[i], rewards[i], next_states[i], dones[i])\n",
    "        \n",
    "        score += rewards\n",
    "        states = next_states\n",
    "        \n",
    "#        maddpg.update_noise()\n",
    "        \n",
    "        if np.any(dones) or  episode_t > max_episode_length:\n",
    "            break \n",
    "        episode_t += 1\n",
    "\n",
    "\n",
    "    max_episode_t = np.max([max_episode_t, episode_t])\n",
    "    # update once after every episode_per_update\n",
    "    # KAE 4/12/22: this section is, for the most part, superceeded by the agent.step\n",
    "    #   function -> agent.learn for each agent\n",
    "    mx_score = np.max(score)\n",
    "    if mx_score >= last_max:\n",
    "        cnt_pos_mxscores += 1\n",
    "        last_max = mx_score\n",
    "    if mx_score > 0.0:\n",
    "        cnt_pos_scores += 1\n",
    "\n",
    "#    print('score: ',mx_score, score)\n",
    "#    print('score: ',mx_score)\n",
    "    scores.append(np.max(score))\n",
    "    mx_scores = np.max(scores)\n",
    "#    print('mx scores:',mx_scores, scores)\n",
    "#    print('mx scores:',mx_scores)\n",
    "\n",
    "#    if episode % avg_interval == 0 or episode == number_of_episodes-1:\n",
    "#        avg_rewards = np.mean(scores)\n",
    "\n",
    "    \n",
    "    nscores_deque.append(mx_score)\n",
    "#    print(type(nscores_deque))\n",
    "#    print('lendq:',len(nscores_deque))\n",
    "#    print('dq:',nscores_deque)\n",
    "#    nscores.append(scores)\n",
    "    min_score = np.min(nscores_deque)\n",
    "    max_score = np.max(nscores_deque)\n",
    "    mean_score = np.mean(scores)\n",
    "    mean_dqscore = np.mean(nscores_deque)\n",
    "    nmean_scores.append(mean_dqscore)\n",
    "    # this gives us the mean score 100 sample mean score for plotting\n",
    "#    nmean_scores.append(mean_score)\n",
    "#    print('\\nEpisode {}\\tAverage Score: {:.4f}, minscores: {:.4f}, maxscores: {:.4f}, DQ_scores: {:.4f}, pk: {}, mxpk: {}, '.\\\n",
    "#          format(episode, mean_score, min_score, max_score, mean_dqscore, cnt_pos_scores, cnt_pos_mxscores), end=\"\")\n",
    "    # THis line remains in place, from the DDPG previous example....\n",
    "#    print('\\rEpisode {}\\tAverage Score: {:.4f}'.format(episode, mean_score), end=\"\")\n",
    "    # only save the current score if it is better than previous save....\n",
    "    # document the ongoing process....\n",
    "    if episode % print_every == 0:\n",
    "#        print('\\rEpisode {} with {} iterations(emx:{}, totmx:{})\\tAverage Score: {:.4f}, minact: {:.4f}, maxact: {:.4f}'.\\\n",
    "#              format(episode, t, episode_t, max_episode_t, mean_score, min_action, max_action), end=\"\")\n",
    "        print('\\nEpisode {}\\tAvg Score: {:.4f}, dqmin: {:.4f}, dqmax: {:.4f}, DQ: {:.4f}/{:.4f}, pk: {}, mxpk: {}'.\\\n",
    "              format(episode, mean_score, min_score, max_score, mean_dqscore, nbest_mean, cnt_pos_scores, cnt_pos_mxscores), end=\"\")\n",
    "#        print('\\nEpisode {}, Average Score: {:.4f}, min score: {:.4f}, max score: {:.4f}'.\\\n",
    "#              format(episode, mean_score, min_score, max_score))\n",
    "    if mean_dqscore > REQ_SCORE:\n",
    "        print('\\nRequirement met on Episode {}, Requirement Average Score: {:.4f}, mxeps{}'.format(episode, mean_score, max_episode_t))\n",
    "    if mean_dqscore > MAX_SCORE:\n",
    "        print('\\nFinal Episode {}, Final Average Score: {:.4f}, mxeps{}'.format(episode, mean_score, max_episode_t))\n",
    "        break\n",
    "        \n",
    "    #saving model\n",
    "    save_dict_list =[]\n",
    "    if save_info or mean_score > nbest_mean:\n",
    "#        if save_info:\n",
    "#            print('\\nNormal save conditions...')\n",
    "        if mean_dqscore > nbest_mean:\n",
    "#        else:\n",
    "#            print('\\nMean score: {:.4f}\\t, old best: {:.4f}, max_episode: {}'.format(mean_score, nbest_mean, max_episode_t))\n",
    "            print('\\nMean score: {:.4f}\\t, old best: {:.4f}, max_episode: {}'.format(mean_dqscore, nbest_mean, max_episode_t))\n",
    "            nbest_mean = mean_dqscore\n",
    "        for i in range(num_agents):\n",
    "            save_dict = {'actor_params' : maddpg_agents[i].actor_local.state_dict(),\n",
    "                         'actor_optim_params': maddpg_agents[i].actor_optimizer.state_dict(),\n",
    "                         'critic_params' : maddpg_agents[i].critic_local.state_dict(),\n",
    "                         'critic_optim_params' : maddpg_agents[i].critic_optimizer.state_dict()}\n",
    "            save_dict_list.append(save_dict)\n",
    "\n",
    "#            torch.save(save_dict_list, 'checkpoint_episode-{}.pt'.format(episode))\n",
    "            torch.save(save_dict_list, 'checkpoint_episode-interim.pt')\n",
    "            \n",
    "print('\\nFinal Best DGave:{:.4f}'.format(nbest_mean))\n",
    "torch.save(save_dict_list, 'checkpoint_episode-final.pt')\n",
    "\n",
    "# moved this up here as for some reason our solution stopped so lost the times\n",
    "tend = datetime.datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDPG learning took  2:30:19.821276 or  1.1274776595  seconds per episode\n"
     ]
    }
   ],
   "source": [
    "dtime = tend - tstart\n",
    "dt_e = float(dtime.total_seconds()) / float(number_of_episodes)\n",
    "print('DDPG learning took ',dtime,'or ',dt_e,' seconds per episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7e07453128>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores,label='scores')\n",
    "plt.plot(np.arange(1, len(nmean_scores)+1), nmean_scores,'r-',\n",
    "         label='mean scores')\n",
    "plt.grid()\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: next section needs to be updated for the saved items above instead of P2 saved items...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDPG total run time took  2:31:21.147958\n"
     ]
    }
   ],
   "source": [
    "tend_run = datetime.datetime.now()\n",
    "dtime_run = tend_run - tstart_run\n",
    "print('DDPG total run time took ',dtime_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: we don't uncomment the line below until things are working...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The final agent solution \n",
    "Techically speaking we made our requirement at 4520 with learning score of 0.5019, however in these cases I typically don't break out until I've exceeded the requirement by some small amount (in this case 10% = 0.55); this however resulted in taking 7910 episodes to reach a score of 0.5430\n",
    "\n",
    "num_instance -- number of instances of agents employed (=1 for this project)\n",
    "random_seed -- random seed\n",
    "lr_actor -- learning rate of the actor (2.0e-4 final value)\n",
    "lr_critic -- learning rate of the critic (2.0e-4, final value)\n",
    "tau -- soft_update size of updating the target network with the local network values (1.0e-2 final value)\n",
    "gamma -- discount factor for the learn method for the critic network (0.8 final value)\n",
    "The ddpg_agent also contains the following hardcoded parameters:\n",
    "Buffer size: 100,000\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.8             # discount factor\n",
    "TAU = 1e-2              # for soft update of target parameters\n",
    "LR_ACTOR = 2e-4         # learning rate of the actor \n",
    "LR_CRITIC = 2e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay used in critic optimizer\n",
    "LEARN_START = 0         # minimum memory in replay buffer before executing learn method\n",
    "UPDATE_EVERY=10         # used to modulo the increasing t_step internal counter to determine if learning from replaybuffer experiences (or not)\n",
    "UPDATES_PER_STEP=10     # define number of times the model learns from the randomly selected experiences in replaybuffer\n",
    "NOISE_MU = 0.0          # the mean of the action noise for OUNoise class\n",
    "NOISE_THETA = 0.15      # the memory term for the Ornstein-Uhlenbeck process\n",
    "NOISE_SIGMA = 0.1       # the standard deviation of action noise for OUNoise class, 0.1 was better than 0.2\n",
    "\n",
    "DEFAULT_FC1_ACTOR = 40 # THIS IS THE ONE SUBMITTED\n",
    "DEFAULT_FC2_ACTOR = 20 # THIS IS THE ONE SUBMITTED\n",
    "DEFAULT_FC1_CRITIC = DEFAULT_FC1_ACTOR\n",
    "DEFAULT_FC2_CRITIC = DEFAULT_FC2_ACTOR\n",
    "\n",
    "However you can see the output of many of the more fixed hyperparameters (not passed via argument list) in the Tennis.ipynb file.\n",
    "\n",
    "# Future proposed additions\n",
    "Like our previous project, we would have liked to potentially explore some of the post-DDPG example approaches, such as MADDPG (Multi-Agent DDPG) and the N-step. However since these were mainly modifications of the internal workings of the agents and the like, we felt that it was best to first get the baseline DDPG running and then see if there are problems about possibly making these modifications. \n",
    "\n",
    "So we start with our original agent and model, which we've imported locally and import the (slightly modified) DDPG function for the unity setup, and this was found to be sufficient for this exercise.\n",
    "\n",
    "As mentioned above we believe probably the greatest improvement will come from an N-step implementation, since this could provide information on longer-term solutions earlier and provide more information to the networks about longer term rewards.\n",
    "\n",
    "Another would be to do at least a much greater sensitivity exploration of the hyperparameters after our BATCHSIZE increase to 128, not only of the hyperparameters that were passed via argument list:\n",
    "\n",
    "LR_ACTOR = 2e-4         # learning rate of the actor \n",
    "LR_CRITIC = 2e-4        # learning rate of the critic\n",
    "TAU = 1e-2              # for soft update of target parameters\n",
    "GAMMA = 0.8            # discount factor\n",
    "\n",
    "But also a number of the more \"hardcoded\" values:\n",
    "\n",
    "DDPG Agent.init, UPDATE_EVERY: 10\n",
    "DDPG Agent.init, UPDATES_PER_STEP: 10\n",
    "DDPG Agent.init, LEARN_START: 0\n",
    "DDPG Agent.init, NOISE_MU: 0.0\n",
    "DDPG Agent.init, NOISE_THETA: 0.15\n",
    "DDPG Agent.init, NOISE_SIGMA: 0.1\n",
    "\n",
    "as well as a number of the various other techniques we tried before that didn't work initially:\n",
    "1) adding noise to the weights in the network INSTEAD of the action\n",
    "2) OR adding noise to the states INSTEAD of the action. \n",
    "3) batch normalization\n",
    "4) dropouts.\n",
    "\n",
    "neither of which we had time at the end to fully expore.\n",
    "\n",
    "\n",
    "# REFERENCES\n",
    "\n",
    "## Basic\n",
    "Github example of continous control with DDPG\n",
    "https://github.com/MariannaJan/continous_control_DDPG\n",
    "\n",
    "Article: \"Continuous Deep Q-Learning with Model-based Acceleration\", by Gu et al., 2016\n",
    "https://arxiv.org/pdf/1603.00748.pdf\n",
    "\n",
    "Article: \"CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING\" by Lillencrap et al, 2019\n",
    "https://arxiv.org/pdf/1509.02971.pdf\n",
    "\n",
    "Article: \"Benchmarking Deep Reinforcement Learning for Continuous Control\" by Duan et al, 2016\n",
    "https://arxiv.org/pdf/1604.06778.pdf\n",
    "\n",
    "\n",
    "\"Lets make a DQN: Double Learning and Prioritized Experience Replay\", 2016\n",
    "https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/\n",
    "\n",
    "\n",
    "Github RL Lab by by ShangtonZhang\n",
    "https://github.com/rll/rllab\n",
    "\n",
    "\"DDPG (Actor-Critic) Reinforcement Learning using PyTorch and Unity ML-Agents\", solution to \n",
    "\"how to handle mutliple agents in ddpg pytorch\" google search\n",
    "https://github.com/xkiwilabs/DDPG-using-PyTorch-and-ML-Agents\n",
    "\n",
    "Examples:\n",
    "https://github.com/Nathan1123/P3_Collaboration_and_Competition\n",
    "https://github.com/ainvyu/p3-collab-compet\n",
    " \n",
    "\n",
    "## D4PG\n",
    "Article: \"DISTRIBUTED DISTRIBUTIONAL DETERMINISTIC POLICY GRADIENTS\",by Barth-Maron, 2018\n",
    "https://openreview.net/pdf?id=SyZipzbCb\n",
    "\n",
    "## General knowledge of various topics in no particular order (as opened I think)\n",
    "https://discuss.pytorch.org/t/is-there-any-way-to-add-noise-to-trained-weights/29829\n",
    "\n",
    "https://www.google.com/search?q=how+to+add+noise+to+hidden+weights+in+neural+network&sxsrf=APq-WBsz6Lkp4NEm56rbGqCPlcJUrUu3Bg%3A1649895248160&ei=UGdXYtW0CdDTkPIPvrylqAE&oq=how+to+add+noise+to+hidden+weights+&gs_lcp=Cgdnd3Mtd2l6EAEYAjIFCCEQoAEyBQghEKABMgUIIRCgAUoECEEYAEoECEYYAFAAWOVcYNJ9aABwAHgAgAGZAYgB4R6SAQUxMi4yNJgBAKABAcABAQ&sclient=gws-wiz#kpvalbx=_XGhXYraXAdmfkPIP79uQ0AQ3\n",
    "\n",
    "https://stackoverflow.com/questions/59013993/how-can-i-add-bias-using-pytorch-to-a-neural-network\n",
    "\n",
    "https://www.codetd.com/en/article/11834682\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n",
    "\n",
    "https://discuss.pytorch.org/t/add-gaussian-noise-to-parameters-while-training/109260\n",
    "\n",
    "https://discuss.pytorch.org/t/is-there-any-way-to-add-noise-to-trained-weights/29829/3\n",
    "\n",
    "https://discuss.pytorch.org/t/how-to-install-pytorch-0-4-0-with-cuda-9-0/48914\n",
    "\n",
    "https://coder.social/udacity/deep-reinforcement-learning\n",
    "\n",
    "https://www.bing.com/search?q=pytorch+0.4.0+cuda&form=ANNTH1&refig=c3d77ee899304f5fabb8f3cedcf82bfe&ntref=1\n",
    "\n",
    "https://discuss.pytorch.org/t/how-to-install-pytorch-0-4-0-with-cuda-9-0/48914\n",
    "\n",
    "https://www.bing.com/search?q=KeyError%3A%20%22Couldn%27t%20find%20field%20FieldDescriptorProto.proto3_optional%22%20python%20unity&qs=n&form=QBRE&=%25eManage%20Your%20Search%20History%25E&sp=-1&pq=keyerror%3A%20%22couldn%27t%20find%20field%20google.protobuf.fielddescriptorproto.proto3_optional%22%20python%20unity&sc=0-97&sk=&cvid=E9ABE10449F64343A0C7058F615BDFE4&ntref=1\n",
    "\n",
    "https://stackoverflow.com/questions/71626781/anaconda-python-expainerdashboard-keyerror-couldnt-find-field-google-pr\n",
    "\n",
    "https://stackoverflow.com/questions/47257751/keyerror-couldnt-find-field-google-protobuf-descriptorproto-extensionrange-op\n",
    "\n",
    "https://www.bing.com/search?q=how%20to%20install%20kernel%20in%20jupyter%20for%20specific%20anaconda%20environment&qs=n&form=QBRE&=%25eManage%20Your%20Search%20History%25E&sp=-1&pq=how%20to%20install%20kernel%20in%20jupyter%20for%20specific%20anaconda%20env&sc=0-58&sk=&cvid=8D1EBF16895D4FAC88936EF9826A2315&ntref=1\n",
    "\n",
    "https://gdcoder.com/how-to-create-and-add-a-conda-environment-as-jupyter-kernel/\n",
    "\n",
    "https://www.bing.com/search?q=how+to+get+a+directory+listing+in+python&form=ANNTH1&refig=7cad7cc5de9d4a9194816ef7db907e47&sp=3&qs=UT&pq=how+to+get+a+directory+listing+&sk=PRES1UT2&sc=8-31&cvid=7cad7cc5de9d4a9194816ef7db907e47\n",
    "\n",
    "https://www.bing.com/search?q=gaierror+python+3&form=ANNTH1&refig=7fa870dbe59b43ac9c3f41d4079b6192&sp=1&qs=MT&pq=gaierror+&sk=PRES1&sc=8-9&cvid=7fa870dbe59b43ac9c3f41d4079b6192&ntref=1\n",
    "\n",
    "https://www.tecmint.com/resolve-temporary-failure-in-name-resolution/\n",
    "\n",
    "https://stackoverflow.com/questions/40238610/what-is-the-meaning-of-gaierror-errno-3-temporary-failure-in-name-resolutio\n",
    "\n",
    "https://www.bing.com/search?q=python%20NewConnectionError%3A%20%3Curllib3.connection.HTTPConnection%20object%20at%200x0000022F86B32160%3E%3A%20Failed%20to%20establish%20a%20new%20connection%3A%20%5BErrno%2011001%5D%20getaddrinfo%20failed&qs=n&form=QBRE&=%25eManage%20Your%20Search%20History%25E&sp=-1&pq=newconnectionerror%3A%20%3Curllib3.connection.httpconnection%20object%20at%200x0000022f86b32160%3E%3A%20failed%20to%20establish%20a%20new%20connection%3A%20%5Berrno%2011001%5D%20getaddrinfo%20failed&sc=0-156&sk=&cvid=ACDA2B21B37E4AF3B44B2A8AA3F0365F&ntref=1\n",
    "\n",
    "https://itecnote.com/tecnote/python-httpconnectionpool-failed-to-establish-a-new-connection-errno-11004-getaddrinfo-failed/\n",
    "\n",
    "https://github.com/openai?msclkid=7f9ebcb6b20011ecb533e3c5de0cedd9\n",
    "\n",
    "https://github.com/llSourcell/OpenAI_Five_vs_Dota2_Explained?msclkid=7f9d4f9db20011eca2d8c4fc80eeec2f\n",
    "\n",
    "https://github.com/tristandeleu/pytorch-maml-rl\n",
    "\n",
    "https://github.com/openai/maddpg\n",
    "\n",
    "https://github.com/rlworkgroup/garage\n",
    "\n",
    "https://github.com/shariqiqbal2810/MAAC\n",
    "\n",
    "https://github.com/openai/gym/wiki/BipedalWalker-v2\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.cat.html\n",
    "\n",
    "https://stackoverflow.com/questions/47331235/how-should-openai-environments-gyms-use-env-seed0\n",
    "\n",
    "http://download.pytorch.org/whl/cpu/torch/\n",
    "\n",
    "https://stackoverflow.com/questions/19283271/how-to-uninstall-requests-2-0-0\n",
    "\n",
    "https://stackoverflow.com/questions/38411942/anaconda-conda-install-a-specific-package-version\n",
    "\n",
    "https://pypi.org/project/mxnet/\n",
    "\n",
    "https://discuss.pytorch.org/t/tensorboard-with-pytorch-summarywritter/84668\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.norm.html\n",
    "\n",
    "https://www.programcreek.com/python/example/107653/torch.nn.BatchNorm1d\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n",
    "\n",
    "https://numpy.org/doc/stable/reference/generated/numpy.ndarray.view.html\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.Tensor.view.html\n",
    "\n",
    "https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-ML-Agents.md#training-using-concurrent-unity-instances\n",
    "\n",
    "https://realpython.com/python-or-operator/\n",
    "\n",
    "https://pytorch.org/get-started/previous-versions/\n",
    "\n",
    "https://developer.nvidia.com/cuda-90-download-archive?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exelocal\n",
    "\n",
    "https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html#download-cuda-software\n",
    "\n",
    "https://developer.nvidia.com/cuda-toolkit\n",
    "\n",
    "https://developer.nvidia.com/search?page=3&sort=relevance&term=using%20cuda%209.0%20python%20on%20windows%2011\n",
    "\n",
    "https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html\n",
    "\n",
    "https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html\n",
    "\n",
    "https://www.google.com/search?q=conda+create+environment+from+yml&sxsrf=APq-WBtkxuAaqkH2fFvJeTBBEKl-6HLc1w%3A1649792235975&ei=69RVYumWO62dkPIP3_6RoA4&oq=conda+env+create+-f&gs_lcp=Cgdnd3Mtd2l6EAEYATIHCAAQRxCwAzIHCAAQRxCwAzIHCAAQRxCwAzIHCAAQRxCwAzIHCAAQRxCwAzIHCAAQRxCwAzIHCAAQRxCwAzIHCAAQRxCwA0oECEEYAEoECEYYAFAAWABgnhpoAXABeACAAQCIAQCSAQCYAQDIAQjAAQE&sclient=gws-wiz\n",
    "\n",
    "https://docs.conda.io/projects/conda-build/en/latest/user-guide/wheel-files.html\n",
    "\n",
    "https://docs.python.org/3/library/copy.html#module-copy\n",
    "\n",
    "https://www.geeksforgeeks.org/check-multiple-conditions-in-if-statement-python/\n",
    "\n",
    "https://stackoverflow.com/questions/52288635/how-do-i-use-torch-stack\n",
    "\n",
    "https://medium.com/adding-noise-to-network-weights-in-tensorflow/adding-noise-to-network-weights-in-tensorflow-fddc82e851cb\n",
    "\n",
    "https://discuss.pytorch.org/t/how-to-change-the-weights-of-a-pytorch-model/41279\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n",
    "\n",
    "https://machinelearningmastery.com/train-neural-networks-with-noise-to-reduce-overfitting/\n",
    "\n",
    "https://www.codetd.com/en/article/11834682\n",
    "\n",
    "https://machinelearningmastery.com/train-neural-networks-with-noise-to-reduce-overfitting/\n",
    "\n",
    "https://discuss.pytorch.org/t/is-there-any-way-to-add-noise-to-trained-weights/29829/9\n",
    "\n",
    "https://arxiv.org/pdf/1805.08000.pdf\n",
    "\n",
    "https://stackoverflow.com/questions/59013993/how-can-i-add-bias-using-pytorch-to-a-neural-network\n",
    "\n",
    "https://discuss.pytorch.org/t/add-gaussian-noise-to-parameters-while-training/109260\n",
    "\n",
    "https://github.com/christianversloot/machine-learning-articles/blob/main/using-dropout-with-pytorch.md\n",
    "\n",
    "https://stackoverflow.com/questions/9413367/most-efficent-way-of-finding-the-minimum-float-in-a-python-list\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
